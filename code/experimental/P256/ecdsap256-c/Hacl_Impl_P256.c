/* 
  This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
  KreMLin invocation: /home/nkulatov/new2/kremlin/kremlin/krml -fbuiltin-uint128 -fnocompound-literals -fc89-scope -fparentheses -fcurly-braces -funroll-loops 4 -warn-error +9 -add-include "kremlib.h" -add-include "FStar_UInt_8_16_32_64.h" /dist/minimal/testlib.c -skip-compilation -no-prefix Hacl.Impl.P256 -bundle Lib.* -bundle Spec.* -bundle C=C.Endianness -bundle Hacl.Hash.SHA2=Hacl.Hash.*,Spec.Hash.* -bundle Hacl.Impl.P256=Hacl.Impl.P256,Hacl.Impl.P256.MontgomeryMultiplication,Hacl.Impl.P256.LowLevel,Hacl.Impl.LowLevel,Hacl.Impl.SolinasReduction,Hacl.Spec.P256.*,Hacl.Spec.Curve25519.*,Hacl.Impl.Curve25519.* -bundle Hacl.Impl.ECDSA.P256SHA256.Verification=Hacl.Impl.MontgomeryMultiplication,Hacl.Impl.ECDSA.P256SHA256.Verification,Hacl.Impl.MM.Exponent -library C,FStar -drop LowStar,Spec,Prims,Lib,C.Loops.*,Hacl.Spec.P256.Lemmas,Hacl.Spec.P256,Hacl.Spec.ECDSA -add-include "c/Lib_PrintBuffer.h" -add-include "FStar_UInt_8_16_32_64.h" -tmpdir ecdsap256-c .output/prims.krml .output/FStar_Pervasives_Native.krml .output/FStar_Pervasives.krml .output/FStar_Squash.krml .output/FStar_Classical.krml .output/FStar_StrongExcludedMiddle.krml .output/FStar_FunctionalExtensionality.krml .output/FStar_List_Tot_Base.krml .output/FStar_List_Tot_Properties.krml .output/FStar_List_Tot.krml .output/FStar_Mul.krml .output/FStar_Math_Lib.krml .output/FStar_Math_Lemmas.krml .output/FStar_Seq_Base.krml .output/FStar_Seq_Properties.krml .output/FStar_Seq.krml .output/FStar_Set.krml .output/FStar_Preorder.krml .output/FStar_Ghost.krml .output/FStar_ErasedLogic.krml .output/FStar_PropositionalExtensionality.krml .output/FStar_PredicateExtensionality.krml .output/FStar_TSet.krml .output/FStar_Monotonic_Heap.krml .output/FStar_Heap.krml .output/FStar_Map.krml .output/FStar_Monotonic_Witnessed.krml .output/FStar_Monotonic_HyperHeap.krml .output/FStar_Monotonic_HyperStack.krml .output/FStar_HyperStack.krml .output/FStar_HyperStack_ST.krml .output/FStar_Calc.krml .output/FStar_BitVector.krml .output/FStar_UInt.krml .output/FStar_UInt32.krml .output/FStar_Universe.krml .output/FStar_GSet.krml .output/FStar_ModifiesGen.krml .output/FStar_Range.krml .output/FStar_Reflection_Types.krml .output/FStar_Tactics_Types.krml .output/FStar_Tactics_Result.krml .output/FStar_Tactics_Effect.krml .output/FStar_Tactics_Util.krml .output/FStar_Reflection_Data.krml .output/FStar_Reflection_Const.krml .output/FStar_Char.krml .output/FStar_Exn.krml .output/FStar_ST.krml .output/FStar_All.krml .output/FStar_List.krml .output/FStar_String.krml .output/FStar_Order.krml .output/FStar_Reflection_Basic.krml .output/FStar_Reflection_Derived.krml .output/FStar_Tactics_Builtins.krml .output/FStar_Reflection_Formula.krml .output/FStar_Reflection_Derived_Lemmas.krml .output/FStar_Reflection.krml .output/FStar_Tactics_Derived.krml .output/FStar_Tactics_Logic.krml .output/FStar_Tactics.krml .output/FStar_BigOps.krml .output/LowStar_Monotonic_Buffer.krml .output/LowStar_Buffer.krml .output/LowStar_BufferOps.krml .output/Spec_Loops.krml .output/FStar_UInt64.krml .output/C_Loops.krml .output/FStar_Int.krml .output/FStar_Int64.krml .output/FStar_Int63.krml .output/FStar_Int32.krml .output/FStar_Int16.krml .output/FStar_Int8.krml .output/FStar_UInt63.krml .output/FStar_UInt16.krml .output/FStar_UInt8.krml .output/FStar_Int_Cast.krml .output/FStar_UInt128.krml .output/FStar_Int_Cast_Full.krml .output/FStar_Int128.krml .output/Lib_IntTypes.krml .output/Lib_Loops.krml .output/Lib_LoopCombinators.krml .output/Lib_RawIntTypes.krml .output/Lib_Sequence.krml .output/Lib_ByteSequence.krml .output/LowStar_ImmutableBuffer.krml .output/Lib_Buffer.krml .output/FStar_HyperStack_All.krml .output/Hacl_Spec_ECDSAP256_Definition.krml .output/Lib_IntTypes_Compatibility.krml .output/Spec_Hash_Definitions.krml .output/Spec_Hash_Lemmas0.krml .output/Spec_Hash_PadFinish.krml .output/Spec_SHA1.krml .output/Spec_MD5.krml .output/Spec_SHA2_Constants.krml .output/Spec_SHA2.krml .output/Spec_Hash.krml .output/Spec_Curve25519_Lemmas.krml .output/FStar_Reflection_Arith.krml .output/FStar_Tactics_Canon.krml .output/Hacl_Spec_P256_Definitions.krml .output/Hacl_Impl_Curve25519_Lemmas.krml .output/Spec_Curve25519.krml .output/Hacl_Spec_Curve25519_Field64_Definition.krml .output/Hacl_Spec_Curve25519_Field64_Lemmas.krml .output/Hacl_Spec_P256_Basic.krml .output/Hacl_Spec_P256_Lemmas.krml .output/Hacl_Spec_P256_Core.krml .output/Hacl_Spec_P256.krml .output/Hacl_Spec_ECDSA.krml .output/Hacl_Impl_LowLevel.krml .output/Hacl_Spec_P256_MontgomeryMultiplication.krml .output/Hacl_Impl_P256_LowLevel.krml .output/Hacl_Spec_P256_SolinasReduction.krml .output/Hacl_Impl_SolinasReduction.krml .output/FStar_Kremlin_Endianness.krml .output/C_Endianness.krml .output/C.krml .output/Lib_ByteBuffer.krml .output/Spec_Hash_Incremental.krml .output/Spec_Hash_Lemmas.krml .output/Hacl_Hash_Lemmas.krml .output/LowStar_Modifies.krml .output/Hacl_Hash_Definitions.krml .output/Hacl_Hash_PadFinish.krml .output/Hacl_Hash_MD.krml .output/Hacl_Impl_ECDSA_MontgomeryMultiplication.krml .output/Hacl_Spec_P256_MontgomeryMultiplication_PointDouble.krml .output/Hacl_Spec_P256_MontgomeryMultiplication_PointAdd.krml .output/Hacl_Spec_P256_Ladder.krml .output/Hacl_Impl_ECDSA_MM_Exponent.krml .output/Hacl_Hash_Core_SHA2_Constants.krml .output/Hacl_Hash_Core_SHA2.krml .output/Hacl_Impl_P256_MontgomeryMultiplication.krml .output/Hacl_Spec_P256_Normalisation.krml .output/Hacl_Impl_P256.krml .output/Hacl_Hash_SHA2.krml .output/Hacl_Impl_ECDSA_P256SHA256_Verification.krml
  F* version: ea91ae8c
  KreMLin version: 27ce15c8
 */

#include "Hacl_Impl_P256.h"

inline K___uint64_t_uint64_t
Hacl_Spec_P256_Basic_addcarry(uint64_t x, uint64_t y, uint64_t cin)
{
  uint64_t res1 = x + cin;
  uint64_t c;
  if (res1 < cin)
  {
    c = (uint64_t)1U;
  }
  else
  {
    c = (uint64_t)0U;
  }
  {
    uint64_t res = res1 + y;
    uint64_t c1;
    if (res < res1)
    {
      c1 = c + (uint64_t)1U;
    }
    else
    {
      c1 = c;
    }
    {
      K___uint64_t_uint64_t lit;
      lit.fst = res;
      lit.snd = c1;
      return lit;
    }
  }
}

static uint64_t
Hacl_Impl_LowLevel_add_carry(uint64_t cin, uint64_t x, uint64_t y, uint64_t *result1)
{
  uint64_t res1 = x + cin;
  uint64_t c;
  if (res1 < cin)
  {
    c = (uint64_t)1U;
  }
  else
  {
    c = (uint64_t)0U;
  }
  {
    uint64_t res = res1 + y;
    uint64_t c1;
    if (res < res1)
    {
      c1 = c + (uint64_t)1U;
    }
    else
    {
      c1 = c;
    }
    result1[0U] = res;
    return c1;
  }
}

static uint64_t Hacl_Impl_LowLevel_add8(uint64_t *x, uint64_t *y, uint64_t *result)
{
  uint64_t *a0 = x;
  uint64_t *a1 = x + (uint32_t)4U;
  uint64_t *b0 = y;
  uint64_t *b1 = y + (uint32_t)4U;
  uint64_t *c0 = result;
  uint64_t *c1 = result + (uint32_t)4U;
  uint64_t *r00 = c0;
  uint64_t *r10 = c0 + (uint32_t)1U;
  uint64_t *r20 = c0 + (uint32_t)2U;
  uint64_t *r30 = c0 + (uint32_t)3U;
  uint64_t cc0 = Hacl_Impl_LowLevel_add_carry((uint64_t)0U, a0[0U], b0[0U], r00);
  uint64_t cc1 = Hacl_Impl_LowLevel_add_carry(cc0, a0[1U], b0[1U], r10);
  uint64_t cc2 = Hacl_Impl_LowLevel_add_carry(cc1, a0[2U], b0[2U], r20);
  uint64_t cc3 = Hacl_Impl_LowLevel_add_carry(cc2, a0[3U], b0[3U], r30);
  uint64_t carry0 = cc3;
  uint64_t *r0 = c1;
  uint64_t *r1 = c1 + (uint32_t)1U;
  uint64_t *r2 = c1 + (uint32_t)2U;
  uint64_t *r3 = c1 + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_add_carry(carry0, a1[0U], b1[0U], r0);
  uint64_t cc10 = Hacl_Impl_LowLevel_add_carry(cc, a1[1U], b1[1U], r1);
  uint64_t cc20 = Hacl_Impl_LowLevel_add_carry(cc10, a1[2U], b1[2U], r2);
  uint64_t cc30 = Hacl_Impl_LowLevel_add_carry(cc20, a1[3U], b1[3U], r3);
  uint64_t carry1 = cc30;
  return carry1;
}

static uint64_t
Hacl_Impl_LowLevel_add4_variables(
  uint64_t *x,
  uint64_t cin,
  uint64_t y0,
  uint64_t y1,
  uint64_t y2,
  uint64_t y3,
  uint64_t *result
)
{
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_add_carry(cin, x[0U], y0, r0);
  uint64_t cc1 = Hacl_Impl_LowLevel_add_carry(cc, x[1U], y1, r1);
  uint64_t cc2 = Hacl_Impl_LowLevel_add_carry(cc1, x[2U], y2, r2);
  uint64_t cc3 = Hacl_Impl_LowLevel_add_carry(cc2, x[3U], y3, r3);
  return cc3;
}

uint64_t Hacl_Impl_LowLevel_sub_borrow(uint64_t cin, uint64_t x, uint64_t y, uint64_t *result1)
{
  uint64_t res = x - y - cin;
  uint64_t c;
  if (cin == (uint64_t)1U)
  {
    if (x <= y)
    {
      c = (uint64_t)1U;
    }
    else
    {
      c = (uint64_t)0U;
    }
  }
  else if (x < y)
  {
    c = (uint64_t)1U;
  }
  else
  {
    c = (uint64_t)0U;
  }
  result1[0U] = res;
  return c;
}

uint64_t Hacl_Impl_LowLevel_sub4_il(uint64_t *x, uint64_t *y, uint64_t *result)
{
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_sub_borrow((uint64_t)0U, x[0U], y[0U], r0);
  uint64_t cc1 = Hacl_Impl_LowLevel_sub_borrow(cc, x[1U], y[1U], r1);
  uint64_t cc2 = Hacl_Impl_LowLevel_sub_borrow(cc1, x[2U], y[2U], r2);
  uint64_t cc3 = Hacl_Impl_LowLevel_sub_borrow(cc2, x[3U], y[3U], r3);
  return cc3;
}

static uint64_t Hacl_Impl_LowLevel_sub4(uint64_t *x, uint64_t *y, uint64_t *result)
{
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_sub_borrow((uint64_t)0U, x[0U], y[0U], r0);
  uint64_t cc1 = Hacl_Impl_LowLevel_sub_borrow(cc, x[1U], y[1U], r1);
  uint64_t cc2 = Hacl_Impl_LowLevel_sub_borrow(cc1, x[2U], y[2U], r2);
  uint64_t cc3 = Hacl_Impl_LowLevel_sub_borrow(cc2, x[3U], y[3U], r3);
  return cc3;
}

inline void Hacl_Impl_LowLevel_mul(uint64_t *f1, uint64_t *r, uint64_t *out)
{
  uint64_t f10 = f1[0U];
  uint64_t f11 = f1[1U];
  uint64_t f12 = f1[2U];
  uint64_t f13 = f1[3U];
  uint64_t r0 = r[0U];
  uint64_t r1 = r[1U];
  uint64_t r2 = r[2U];
  uint64_t r3 = r[3U];
  uint128_t res0 = (uint128_t)r0 * f10;
  uint64_t l00 = (uint64_t)res0;
  uint64_t h00 = (uint64_t)(res0 >> (uint32_t)64U);
  uint128_t res1 = (uint128_t)r1 * f10;
  uint64_t l10 = (uint64_t)res1;
  uint64_t h10 = (uint64_t)(res1 >> (uint32_t)64U);
  uint128_t res2 = (uint128_t)r2 * f10;
  uint64_t l20 = (uint64_t)res2;
  uint64_t h20 = (uint64_t)(res2 >> (uint32_t)64U);
  uint128_t res3 = (uint128_t)r3 * f10;
  uint64_t l30 = (uint64_t)res3;
  uint64_t h30 = (uint64_t)(res3 >> (uint32_t)64U);
  uint64_t o04 = l00;
  K___uint64_t_uint64_t scrut0 = Hacl_Spec_P256_Basic_addcarry(l10, h00, (uint64_t)0U);
  uint64_t o10 = scrut0.fst;
  uint64_t c00 = scrut0.snd;
  K___uint64_t_uint64_t scrut1 = Hacl_Spec_P256_Basic_addcarry(l20, h10, c00);
  uint64_t o20 = scrut1.fst;
  uint64_t c10 = scrut1.snd;
  K___uint64_t_uint64_t scrut2 = Hacl_Spec_P256_Basic_addcarry(l30, h20, c10);
  uint64_t o30 = scrut2.fst;
  uint64_t c20 = scrut2.snd;
  uint64_t c30 = h30 + c20;
  K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit0;
  K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut3;
  uint64_t o03;
  uint64_t o02;
  uint64_t o01;
  uint64_t o00;
  uint64_t c0;
  uint128_t res4;
  uint64_t l01;
  uint64_t h01;
  uint128_t res5;
  uint64_t l11;
  uint64_t h11;
  uint128_t res6;
  uint64_t l21;
  uint64_t h21;
  uint128_t res7;
  uint64_t l31;
  uint64_t h31;
  uint64_t o05;
  K___uint64_t_uint64_t scrut4;
  uint64_t o15;
  uint64_t c010;
  K___uint64_t_uint64_t scrut5;
  uint64_t o21;
  uint64_t c12;
  K___uint64_t_uint64_t scrut6;
  uint64_t o31;
  uint64_t c22;
  uint64_t c31;
  lit0.fst = c30;
  lit0.snd.fst = o04;
  lit0.snd.snd = o10;
  lit0.snd.thd = o20;
  lit0.snd.f3 = o30;
  scrut3 = lit0;
  o03 = scrut3.snd.f3;
  o02 = scrut3.snd.thd;
  o01 = scrut3.snd.snd;
  o00 = scrut3.snd.fst;
  c0 = scrut3.fst;
  res4 = (uint128_t)r0 * f11;
  l01 = (uint64_t)res4;
  h01 = (uint64_t)(res4 >> (uint32_t)64U);
  res5 = (uint128_t)r1 * f11;
  l11 = (uint64_t)res5;
  h11 = (uint64_t)(res5 >> (uint32_t)64U);
  res6 = (uint128_t)r2 * f11;
  l21 = (uint64_t)res6;
  h21 = (uint64_t)(res6 >> (uint32_t)64U);
  res7 = (uint128_t)r3 * f11;
  l31 = (uint64_t)res7;
  h31 = (uint64_t)(res7 >> (uint32_t)64U);
  o05 = l01;
  scrut4 = Hacl_Spec_P256_Basic_addcarry(l11, h01, (uint64_t)0U);
  o15 = scrut4.fst;
  c010 = scrut4.snd;
  scrut5 = Hacl_Spec_P256_Basic_addcarry(l21, h11, c010);
  o21 = scrut5.fst;
  c12 = scrut5.snd;
  scrut6 = Hacl_Spec_P256_Basic_addcarry(l31, h21, c12);
  o31 = scrut6.fst;
  c22 = scrut6.snd;
  c31 = h31 + c22;
  {
    K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit1;
    K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut7;
    uint64_t c5;
    K___uint64_t_uint64_t_uint64_t_uint64_t out00;
    uint64_t o06;
    uint64_t o16;
    uint64_t o26;
    uint64_t o32;
    uint64_t f300;
    uint64_t f310;
    uint64_t f320;
    uint64_t f330;
    K___uint64_t_uint64_t scrut8;
    uint64_t o0_;
    uint64_t c011;
    K___uint64_t_uint64_t scrut9;
    uint64_t o1_;
    uint64_t c13;
    K___uint64_t_uint64_t scrut10;
    uint64_t o2_;
    uint64_t c23;
    K___uint64_t_uint64_t scrut11;
    uint64_t o3_;
    uint64_t c32;
    lit1.fst = c31;
    lit1.snd.fst = o05;
    lit1.snd.snd = o15;
    lit1.snd.thd = o21;
    lit1.snd.f3 = o31;
    scrut7 = lit1;
    c5 = scrut7.fst;
    out00 = scrut7.snd;
    o06 = out00.fst;
    o16 = out00.snd;
    o26 = out00.thd;
    o32 = out00.f3;
    f300 = o01;
    f310 = o02;
    f320 = o03;
    f330 = c0;
    scrut8 = Hacl_Spec_P256_Basic_addcarry(f300, o06, (uint64_t)0U);
    o0_ = scrut8.fst;
    c011 = scrut8.snd;
    scrut9 = Hacl_Spec_P256_Basic_addcarry(f310, o16, c011);
    o1_ = scrut9.fst;
    c13 = scrut9.snd;
    scrut10 = Hacl_Spec_P256_Basic_addcarry(f320, o26, c13);
    o2_ = scrut10.fst;
    c23 = scrut10.snd;
    scrut11 = Hacl_Spec_P256_Basic_addcarry(f330, o32, c23);
    o3_ = scrut11.fst;
    c32 = scrut11.snd;
    {
      K___uint64_t_uint64_t_uint64_t_uint64_t out10;
      uint64_t c40;
      out10.fst = o0_;
      out10.snd = o1_;
      out10.thd = o2_;
      out10.f3 = o3_;
      c40 = c5 + c32;
      {
        K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit2;
        K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut12;
        uint64_t o14;
        uint64_t o13;
        uint64_t o12;
        uint64_t o11;
        uint64_t c1;
        uint128_t res8;
        uint64_t l02;
        uint64_t h02;
        uint128_t res9;
        uint64_t l12;
        uint64_t h12;
        uint128_t res10;
        uint64_t l22;
        uint64_t h22;
        uint128_t res11;
        uint64_t l32;
        uint64_t h32;
        uint64_t o07;
        K___uint64_t_uint64_t scrut13;
        uint64_t o17;
        uint64_t c012;
        K___uint64_t_uint64_t scrut14;
        uint64_t o27;
        uint64_t c110;
        K___uint64_t_uint64_t scrut15;
        uint64_t o37;
        uint64_t c24;
        uint64_t c33;
        lit2.fst = c40;
        lit2.snd = out10;
        scrut12 = lit2;
        o14 = scrut12.snd.f3;
        o13 = scrut12.snd.thd;
        o12 = scrut12.snd.snd;
        o11 = scrut12.snd.fst;
        c1 = scrut12.fst;
        res8 = (uint128_t)r0 * f12;
        l02 = (uint64_t)res8;
        h02 = (uint64_t)(res8 >> (uint32_t)64U);
        res9 = (uint128_t)r1 * f12;
        l12 = (uint64_t)res9;
        h12 = (uint64_t)(res9 >> (uint32_t)64U);
        res10 = (uint128_t)r2 * f12;
        l22 = (uint64_t)res10;
        h22 = (uint64_t)(res10 >> (uint32_t)64U);
        res11 = (uint128_t)r3 * f12;
        l32 = (uint64_t)res11;
        h32 = (uint64_t)(res11 >> (uint32_t)64U);
        o07 = l02;
        scrut13 = Hacl_Spec_P256_Basic_addcarry(l12, h02, (uint64_t)0U);
        o17 = scrut13.fst;
        c012 = scrut13.snd;
        scrut14 = Hacl_Spec_P256_Basic_addcarry(l22, h12, c012);
        o27 = scrut14.fst;
        c110 = scrut14.snd;
        scrut15 = Hacl_Spec_P256_Basic_addcarry(l32, h22, c110);
        o37 = scrut15.fst;
        c24 = scrut15.snd;
        c33 = h32 + c24;
        {
          K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit3;
          K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut16;
          uint64_t c6;
          K___uint64_t_uint64_t_uint64_t_uint64_t out01;
          uint64_t o08;
          uint64_t o18;
          uint64_t o28;
          uint64_t o38;
          uint64_t f301;
          uint64_t f311;
          uint64_t f321;
          uint64_t f331;
          K___uint64_t_uint64_t scrut17;
          uint64_t o0_0;
          uint64_t c013;
          K___uint64_t_uint64_t scrut18;
          uint64_t o1_0;
          uint64_t c111;
          K___uint64_t_uint64_t scrut19;
          uint64_t o2_0;
          uint64_t c25;
          K___uint64_t_uint64_t scrut20;
          uint64_t o3_0;
          uint64_t c34;
          lit3.fst = c33;
          lit3.snd.fst = o07;
          lit3.snd.snd = o17;
          lit3.snd.thd = o27;
          lit3.snd.f3 = o37;
          scrut16 = lit3;
          c6 = scrut16.fst;
          out01 = scrut16.snd;
          o08 = out01.fst;
          o18 = out01.snd;
          o28 = out01.thd;
          o38 = out01.f3;
          f301 = o12;
          f311 = o13;
          f321 = o14;
          f331 = c1;
          scrut17 = Hacl_Spec_P256_Basic_addcarry(f301, o08, (uint64_t)0U);
          o0_0 = scrut17.fst;
          c013 = scrut17.snd;
          scrut18 = Hacl_Spec_P256_Basic_addcarry(f311, o18, c013);
          o1_0 = scrut18.fst;
          c111 = scrut18.snd;
          scrut19 = Hacl_Spec_P256_Basic_addcarry(f321, o28, c111);
          o2_0 = scrut19.fst;
          c25 = scrut19.snd;
          scrut20 = Hacl_Spec_P256_Basic_addcarry(f331, o38, c25);
          o3_0 = scrut20.fst;
          c34 = scrut20.snd;
          {
            K___uint64_t_uint64_t_uint64_t_uint64_t out11;
            uint64_t c41;
            out11.fst = o0_0;
            out11.snd = o1_0;
            out11.thd = o2_0;
            out11.f3 = o3_0;
            c41 = c6 + c34;
            {
              K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit4;
              K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut21;
              uint64_t o25;
              uint64_t o24;
              uint64_t o23;
              uint64_t o22;
              uint64_t c2;
              uint128_t res12;
              uint64_t l0;
              uint64_t h0;
              uint128_t res13;
              uint64_t l1;
              uint64_t h1;
              uint128_t res14;
              uint64_t l2;
              uint64_t h2;
              uint128_t res;
              uint64_t l3;
              uint64_t h3;
              uint64_t o09;
              K___uint64_t_uint64_t scrut22;
              uint64_t o19;
              uint64_t c014;
              K___uint64_t_uint64_t scrut23;
              uint64_t o29;
              uint64_t c112;
              K___uint64_t_uint64_t scrut24;
              uint64_t o39;
              uint64_t c210;
              uint64_t c35;
              lit4.fst = c41;
              lit4.snd = out11;
              scrut21 = lit4;
              o25 = scrut21.snd.f3;
              o24 = scrut21.snd.thd;
              o23 = scrut21.snd.snd;
              o22 = scrut21.snd.fst;
              c2 = scrut21.fst;
              res12 = (uint128_t)r0 * f13;
              l0 = (uint64_t)res12;
              h0 = (uint64_t)(res12 >> (uint32_t)64U);
              res13 = (uint128_t)r1 * f13;
              l1 = (uint64_t)res13;
              h1 = (uint64_t)(res13 >> (uint32_t)64U);
              res14 = (uint128_t)r2 * f13;
              l2 = (uint64_t)res14;
              h2 = (uint64_t)(res14 >> (uint32_t)64U);
              res = (uint128_t)r3 * f13;
              l3 = (uint64_t)res;
              h3 = (uint64_t)(res >> (uint32_t)64U);
              o09 = l0;
              scrut22 = Hacl_Spec_P256_Basic_addcarry(l1, h0, (uint64_t)0U);
              o19 = scrut22.fst;
              c014 = scrut22.snd;
              scrut23 = Hacl_Spec_P256_Basic_addcarry(l2, h1, c014);
              o29 = scrut23.fst;
              c112 = scrut23.snd;
              scrut24 = Hacl_Spec_P256_Basic_addcarry(l3, h2, c112);
              o39 = scrut24.fst;
              c210 = scrut24.snd;
              c35 = h3 + c210;
              {
                K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit5;
                K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut25;
                uint64_t c;
                K___uint64_t_uint64_t_uint64_t_uint64_t out0;
                uint64_t o010;
                uint64_t o110;
                uint64_t o210;
                uint64_t o310;
                uint64_t f30;
                uint64_t f31;
                uint64_t f32;
                uint64_t f33;
                K___uint64_t_uint64_t scrut26;
                uint64_t o0_1;
                uint64_t c01;
                K___uint64_t_uint64_t scrut27;
                uint64_t o1_1;
                uint64_t c11;
                K___uint64_t_uint64_t scrut28;
                uint64_t o2_1;
                uint64_t c21;
                K___uint64_t_uint64_t scrut29;
                uint64_t o3_1;
                uint64_t c36;
                lit5.fst = c35;
                lit5.snd.fst = o09;
                lit5.snd.snd = o19;
                lit5.snd.thd = o29;
                lit5.snd.f3 = o39;
                scrut25 = lit5;
                c = scrut25.fst;
                out0 = scrut25.snd;
                o010 = out0.fst;
                o110 = out0.snd;
                o210 = out0.thd;
                o310 = out0.f3;
                f30 = o23;
                f31 = o24;
                f32 = o25;
                f33 = c2;
                scrut26 = Hacl_Spec_P256_Basic_addcarry(f30, o010, (uint64_t)0U);
                o0_1 = scrut26.fst;
                c01 = scrut26.snd;
                scrut27 = Hacl_Spec_P256_Basic_addcarry(f31, o110, c01);
                o1_1 = scrut27.fst;
                c11 = scrut27.snd;
                scrut28 = Hacl_Spec_P256_Basic_addcarry(f32, o210, c11);
                o2_1 = scrut28.fst;
                c21 = scrut28.snd;
                scrut29 = Hacl_Spec_P256_Basic_addcarry(f33, o310, c21);
                o3_1 = scrut29.fst;
                c36 = scrut29.snd;
                {
                  K___uint64_t_uint64_t_uint64_t_uint64_t out1;
                  uint64_t c4;
                  out1.fst = o0_1;
                  out1.snd = o1_1;
                  out1.thd = o2_1;
                  out1.f3 = o3_1;
                  c4 = c + c36;
                  {
                    K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit6;
                    K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut30;
                    uint64_t o36;
                    uint64_t o35;
                    uint64_t o34;
                    uint64_t o33;
                    uint64_t c3;
                    lit6.fst = c4;
                    lit6.snd = out1;
                    scrut30 = lit6;
                    o36 = scrut30.snd.f3;
                    o35 = scrut30.snd.thd;
                    o34 = scrut30.snd.snd;
                    o33 = scrut30.snd.fst;
                    c3 = scrut30.fst;
                    {
                      K___uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t
                      lit;
                      K___uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t
                      scrut;
                      uint64_t o0;
                      uint64_t o1;
                      uint64_t o2;
                      uint64_t o3;
                      uint64_t o4;
                      uint64_t o5;
                      uint64_t o6;
                      uint64_t o7;
                      lit.fst = o00;
                      lit.snd = o11;
                      lit.thd = o22;
                      lit.f3 = o33;
                      lit.f4 = o34;
                      lit.f5 = o35;
                      lit.f6 = o36;
                      lit.f7 = c3;
                      scrut = lit;
                      o0 = scrut.fst;
                      o1 = scrut.snd;
                      o2 = scrut.thd;
                      o3 = scrut.f3;
                      o4 = scrut.f4;
                      o5 = scrut.f5;
                      o6 = scrut.f6;
                      o7 = scrut.f7;
                      out[0U] = o0;
                      out[1U] = o1;
                      out[2U] = o2;
                      out[3U] = o3;
                      out[4U] = o4;
                      out[5U] = o5;
                      out[6U] = o6;
                      out[7U] = o7;
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}

void Hacl_Impl_LowLevel_cmovznz4(uint64_t cin, uint64_t *x, uint64_t *y, uint64_t *r)
{
  uint64_t mask = ~FStar_UInt64_eq_mask(cin, (uint64_t)0U);
  uint64_t r0 = (y[0U] & mask) | (x[0U] & ~mask);
  uint64_t r1 = (y[1U] & mask) | (x[1U] & ~mask);
  uint64_t r2 = (y[2U] & mask) | (x[2U] & ~mask);
  uint64_t r3 = (y[3U] & mask) | (x[3U] & ~mask);
  r[0U] = r0;
  r[1U] = r1;
  r[2U] = r2;
  r[3U] = r3;
}

static void Hacl_Impl_LowLevel_shift_256_impl(uint64_t *i, uint64_t *o)
{
  o[0U] = (uint64_t)0U;
  o[1U] = (uint64_t)0U;
  o[2U] = (uint64_t)0U;
  o[3U] = (uint64_t)0U;
  o[4U] = i[0U];
  o[5U] = i[1U];
  o[6U] = i[2U];
  o[7U] = i[3U];
}

void Hacl_Impl_LowLevel_shortened_mul(uint64_t *a, uint64_t b, uint64_t *result)
{
  uint64_t a0 = a[0U];
  uint64_t a1 = a[1U];
  uint64_t a2 = a[2U];
  uint64_t a3 = a[3U];
  uint128_t res0 = (uint128_t)a0 * b;
  uint64_t l0 = (uint64_t)res0;
  uint64_t h0 = (uint64_t)(res0 >> (uint32_t)64U);
  uint128_t res1 = (uint128_t)a1 * b;
  uint64_t l1 = (uint64_t)res1;
  uint64_t h1 = (uint64_t)(res1 >> (uint32_t)64U);
  uint128_t res2 = (uint128_t)a2 * b;
  uint64_t l2 = (uint64_t)res2;
  uint64_t h2 = (uint64_t)(res2 >> (uint32_t)64U);
  uint128_t res = (uint128_t)a3 * b;
  uint64_t l3 = (uint64_t)res;
  uint64_t h3 = (uint64_t)(res >> (uint32_t)64U);
  uint64_t o0 = l0;
  K___uint64_t_uint64_t scrut0 = Hacl_Spec_P256_Basic_addcarry(l1, h0, (uint64_t)0U);
  uint64_t o1 = scrut0.fst;
  uint64_t c0 = scrut0.snd;
  K___uint64_t_uint64_t scrut1 = Hacl_Spec_P256_Basic_addcarry(l2, h1, c0);
  uint64_t o2 = scrut1.fst;
  uint64_t c1 = scrut1.snd;
  K___uint64_t_uint64_t scrut2 = Hacl_Spec_P256_Basic_addcarry(l3, h2, c1);
  uint64_t o3 = scrut2.fst;
  uint64_t c2 = scrut2.snd;
  uint64_t c3 = h3 + c2;
  K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t lit0;
  K___uint64_t_K___uint64_t_uint64_t_uint64_t_uint64_t scrut3;
  uint64_t f30;
  uint64_t f20;
  uint64_t f10;
  uint64_t f00;
  uint64_t c;
  lit0.fst = c3;
  lit0.snd.fst = o0;
  lit0.snd.snd = o1;
  lit0.snd.thd = o2;
  lit0.snd.f3 = o3;
  scrut3 = lit0;
  f30 = scrut3.snd.f3;
  f20 = scrut3.snd.thd;
  f10 = scrut3.snd.snd;
  f00 = scrut3.snd.fst;
  c = scrut3.fst;
  {
    K___uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t lit;
    K___uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t_uint64_t scrut;
    uint64_t f0;
    uint64_t f1;
    uint64_t f2;
    uint64_t f3;
    uint64_t f4;
    uint64_t f5;
    uint64_t f6;
    uint64_t f7;
    lit.fst = f00;
    lit.snd = f10;
    lit.thd = f20;
    lit.f3 = f30;
    lit.f4 = c;
    lit.f5 = (uint64_t)0U;
    lit.f6 = (uint64_t)0U;
    lit.f7 = (uint64_t)0U;
    scrut = lit;
    f0 = scrut.fst;
    f1 = scrut.snd;
    f2 = scrut.thd;
    f3 = scrut.f3;
    f4 = scrut.f4;
    f5 = scrut.f5;
    f6 = scrut.f6;
    f7 = scrut.f7;
    result[0U] = f0;
    result[1U] = f1;
    result[2U] = f2;
    result[3U] = f3;
    result[4U] = f4;
    result[5U] = f5;
    result[6U] = f6;
    result[7U] = f7;
  }
}

void Hacl_Impl_LowLevel_shift8(uint64_t *t, uint64_t *out)
{
  uint64_t t1 = t[1U];
  uint64_t t2 = t[2U];
  uint64_t t3 = t[3U];
  uint64_t t4 = t[4U];
  uint64_t t5 = t[5U];
  uint64_t t6 = t[6U];
  uint64_t t7 = t[7U];
  out[0U] = t1;
  out[1U] = t2;
  out[2U] = t3;
  out[3U] = t4;
  out[4U] = t5;
  out[5U] = t6;
  out[6U] = t7;
  out[7U] = (uint64_t)0U;
}

uint64_t
Hacl_Impl_P256_LowLevel_prime256_buffer[4U] =
  {
    (uint64_t)0xffffffffffffffffU,
    (uint64_t)0xffffffffU,
    (uint64_t)0U,
    (uint64_t)0xffffffff00000001U
  };

static void Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(uint64_t *x, uint64_t *result)
{
  uint64_t tempBuffer[4U] = { 0U };
  uint64_t
  c = Hacl_Impl_LowLevel_sub4_il(x, Hacl_Impl_P256_LowLevel_prime256_buffer, tempBuffer);
  Hacl_Impl_LowLevel_cmovznz4(c, tempBuffer, x, result);
}

static void Hacl_Impl_P256_LowLevel_p256_add(uint64_t *arg1, uint64_t *arg2, uint64_t *out)
{
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_add_carry((uint64_t)0U, arg1[0U], arg2[0U], r0);
  uint64_t cc1 = Hacl_Impl_LowLevel_add_carry(cc, arg1[1U], arg2[1U], r1);
  uint64_t cc2 = Hacl_Impl_LowLevel_add_carry(cc1, arg1[2U], arg2[2U], r2);
  uint64_t cc3 = Hacl_Impl_LowLevel_add_carry(cc2, arg1[3U], arg2[3U], r3);
  uint64_t t = cc3;
  uint64_t tempBuffer[4U] = { 0U };
  uint64_t tempBufferForSubborrow = (uint64_t)0U;
  uint64_t
  c = Hacl_Impl_LowLevel_sub4_il(out, Hacl_Impl_P256_LowLevel_prime256_buffer, tempBuffer);
  uint64_t carry = Hacl_Impl_LowLevel_sub_borrow(c, t, (uint64_t)0U, &tempBufferForSubborrow);
  Hacl_Impl_LowLevel_cmovznz4(carry, tempBuffer, out, out);
}

static void Hacl_Impl_P256_LowLevel_p256_double(uint64_t *arg1, uint64_t *out)
{
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_LowLevel_add_carry((uint64_t)0U, arg1[0U], arg1[0U], r0);
  uint64_t cc1 = Hacl_Impl_LowLevel_add_carry(cc, arg1[1U], arg1[1U], r1);
  uint64_t cc2 = Hacl_Impl_LowLevel_add_carry(cc1, arg1[2U], arg1[2U], r2);
  uint64_t cc3 = Hacl_Impl_LowLevel_add_carry(cc2, arg1[3U], arg1[3U], r3);
  uint64_t t = cc3;
  uint64_t tempBuffer[4U] = { 0U };
  uint64_t tempBufferForSubborrow = (uint64_t)0U;
  uint64_t
  c = Hacl_Impl_LowLevel_sub4_il(out, Hacl_Impl_P256_LowLevel_prime256_buffer, tempBuffer);
  uint64_t carry = Hacl_Impl_LowLevel_sub_borrow(c, t, (uint64_t)0U, &tempBufferForSubborrow);
  Hacl_Impl_LowLevel_cmovznz4(carry, tempBuffer, out, out);
}

static void Hacl_Impl_P256_LowLevel_p256_sub(uint64_t *arg1, uint64_t *arg2, uint64_t *out)
{
  uint64_t t = Hacl_Impl_LowLevel_sub4(arg1, arg2, out);
  uint64_t t0 = (uint64_t)0U - t;
  uint64_t t1 = ((uint64_t)0U - t) >> (uint32_t)32U;
  uint64_t t2 = (uint64_t)0U;
  uint64_t t3 = t - (t << (uint32_t)32U);
  uint64_t c = Hacl_Impl_LowLevel_add4_variables(out, (uint64_t)0U, t0, t1, t2, t3, out);
}

static uint64_t Hacl_Impl_SolinasReduction_store_high_low_u(uint32_t high, uint32_t low)
{
  uint64_t as_uint64_high = (uint64_t)high;
  uint64_t as_uint64_high1 = as_uint64_high << (uint32_t)32U;
  uint64_t as_uint64_low = (uint64_t)low;
  return as_uint64_low ^ as_uint64_high1;
}

static void
Hacl_Impl_SolinasReduction_upl_zer_buffer(
  uint32_t c0,
  uint32_t c1,
  uint32_t c2,
  uint32_t c3,
  uint32_t c4,
  uint32_t c5,
  uint32_t c6,
  uint32_t c7,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c1, c0);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c3, c2);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c5, c4);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c7, c6);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_fir_buffer(
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = (uint64_t)0U;
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c11, (uint32_t)0U);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c13, c12);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c15, c14);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_sec_buffer(
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = (uint64_t)0U;
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c12, (uint32_t)0U);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c14, c13);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u((uint32_t)0U, c15);
  o[0U] = b0;
  o[1U] = b1;
  o[2U] = b2;
  o[3U] = b3;
}

static void
Hacl_Impl_SolinasReduction_upl_thi_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c9, c8);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u((uint32_t)0U, c10);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c15, c14);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_for_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c11,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c10, c9);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c13, c11);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c15, c14);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c8, c13);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_fif_buffer(
  uint32_t c8,
  uint32_t c10,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c12, c11);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u((uint32_t)0U, c13);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c10, c8);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_six_buffer(
  uint32_t c9,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c13, c12);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c15, c14);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c11, c9);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_sev_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c14, c13);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c8, c15);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c10, c9);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c12, (uint32_t)0U);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void
Hacl_Impl_SolinasReduction_upl_eig_buffer(
  uint32_t c9,
  uint32_t c10,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Impl_SolinasReduction_store_high_low_u(c15, c14);
  uint64_t b1 = Hacl_Impl_SolinasReduction_store_high_low_u(c9, (uint32_t)0U);
  uint64_t b2 = Hacl_Impl_SolinasReduction_store_high_low_u(c11, c10);
  uint64_t b3 = Hacl_Impl_SolinasReduction_store_high_low_u(c13, (uint32_t)0U);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_P256_LowLevel_reduction_prime_2prime_impl(temp, o);
}

static void Hacl_Impl_SolinasReduction_solinas_reduction_impl(uint64_t *i, uint64_t *o)
{
  uint64_t tempBuffer[36U] = { 0U };
  uint64_t i0 = i[0U];
  uint64_t i1 = i[1U];
  uint64_t i2 = i[2U];
  uint64_t i3 = i[3U];
  uint64_t i4 = i[4U];
  uint64_t i5 = i[5U];
  uint64_t i6 = i[6U];
  uint64_t i7 = i[7U];
  uint32_t c0 = (uint32_t)i0;
  uint32_t c1 = (uint32_t)(i0 >> (uint32_t)32U);
  uint32_t c2 = (uint32_t)i1;
  uint32_t c3 = (uint32_t)(i1 >> (uint32_t)32U);
  uint32_t c4 = (uint32_t)i2;
  uint32_t c5 = (uint32_t)(i2 >> (uint32_t)32U);
  uint32_t c6 = (uint32_t)i3;
  uint32_t c7 = (uint32_t)(i3 >> (uint32_t)32U);
  uint32_t c8 = (uint32_t)i4;
  uint32_t c9 = (uint32_t)(i4 >> (uint32_t)32U);
  uint32_t c10 = (uint32_t)i5;
  uint32_t c11 = (uint32_t)(i5 >> (uint32_t)32U);
  uint32_t c12 = (uint32_t)i6;
  uint32_t c13 = (uint32_t)(i6 >> (uint32_t)32U);
  uint32_t c14 = (uint32_t)i7;
  uint32_t c15 = (uint32_t)(i7 >> (uint32_t)32U);
  uint64_t redBuffer[4U] = { 0U };
  uint64_t *t610 = tempBuffer + (uint32_t)24U;
  uint64_t *t710 = tempBuffer + (uint32_t)28U;
  uint64_t *t810 = tempBuffer + (uint32_t)32U;
  uint64_t *t010 = tempBuffer;
  uint64_t *t110 = tempBuffer + (uint32_t)4U;
  uint64_t *t210 = tempBuffer + (uint32_t)8U;
  uint64_t *t310 = tempBuffer + (uint32_t)12U;
  uint64_t *t410 = tempBuffer + (uint32_t)16U;
  uint64_t *t510 = tempBuffer + (uint32_t)20U;
  uint64_t *t01;
  uint64_t *t11;
  uint64_t *t21;
  uint64_t *t31;
  uint64_t *t41;
  uint64_t *t51;
  uint64_t *t61;
  uint64_t *t71;
  uint64_t *t81;
  Hacl_Impl_SolinasReduction_upl_zer_buffer(c0, c1, c2, c3, c4, c5, c6, c7, redBuffer, t010);
  Hacl_Impl_SolinasReduction_upl_fir_buffer(c11, c12, c13, c14, c15, redBuffer, t110);
  Hacl_Impl_SolinasReduction_upl_sec_buffer(c12, c13, c14, c15, redBuffer, t210);
  Hacl_Impl_SolinasReduction_upl_thi_buffer(c8, c9, c10, c14, c15, redBuffer, t310);
  Hacl_Impl_SolinasReduction_upl_for_buffer(c8, c9, c10, c11, c13, c14, c15, redBuffer, t410);
  Hacl_Impl_SolinasReduction_upl_fif_buffer(c8, c10, c11, c12, c13, redBuffer, t510);
  Hacl_Impl_SolinasReduction_upl_six_buffer(c9, c11, c12, c13, c14, c15, redBuffer, t610);
  Hacl_Impl_SolinasReduction_upl_sev_buffer(c8, c9, c10, c12, c13, c14, c15, redBuffer, t710);
  Hacl_Impl_SolinasReduction_upl_eig_buffer(c9, c10, c11, c12, c13, c14, c15, redBuffer, t810);
  t01 = tempBuffer;
  t11 = tempBuffer + (uint32_t)4U;
  t21 = tempBuffer + (uint32_t)8U;
  t31 = tempBuffer + (uint32_t)12U;
  t41 = tempBuffer + (uint32_t)16U;
  t51 = tempBuffer + (uint32_t)20U;
  t61 = tempBuffer + (uint32_t)24U;
  t71 = tempBuffer + (uint32_t)28U;
  t81 = tempBuffer + (uint32_t)32U;
  Hacl_Impl_P256_LowLevel_p256_double(t21, t21);
  Hacl_Impl_P256_LowLevel_p256_double(t11, t11);
  Hacl_Impl_P256_LowLevel_p256_add(t01, t11, o);
  Hacl_Impl_P256_LowLevel_p256_add(t21, o, o);
  Hacl_Impl_P256_LowLevel_p256_add(t31, o, o);
  Hacl_Impl_P256_LowLevel_p256_add(t41, o, o);
  Hacl_Impl_P256_LowLevel_p256_sub(o, t51, o);
  Hacl_Impl_P256_LowLevel_p256_sub(o, t61, o);
  Hacl_Impl_P256_LowLevel_p256_sub(o, t71, o);
  Hacl_Impl_P256_LowLevel_p256_sub(o, t81, o);
}

static void Hacl_Spec_P256_Ladder_cswap(uint64_t bit, uint64_t *p1, uint64_t *p2)
{
  uint64_t mask = (uint64_t)0U - bit;
  uint32_t i;
  for (i = (uint32_t)0U; i < (uint32_t)12U; i = i + (uint32_t)1U)
  {
    uint64_t dummy = mask & (p1[i] ^ p2[i]);
    p1[i] = p1[i] ^ dummy;
    p2[i] = p2[i] ^ dummy;
  }
}

static void
Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(
  uint64_t *a,
  uint64_t *b,
  uint64_t *result
)
{
  uint64_t t[8U] = { 0U };
  uint64_t round2[8U] = { 0U };
  uint64_t round4[8U] = { 0U };
  Hacl_Impl_LowLevel_mul(a, b, t);
  {
    uint64_t tempRound[8U] = { 0U };
    uint64_t t20[8U] = { 0U };
    uint64_t t30[8U] = { 0U };
    uint64_t t10 = t[0U];
    uint64_t uu____0;
    Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t10, t20);
    uu____0 = Hacl_Impl_LowLevel_add8(t, t20, t30);
    Hacl_Impl_LowLevel_shift8(t30, tempRound);
    {
      uint64_t t21[8U] = { 0U };
      uint64_t t31[8U] = { 0U };
      uint64_t t11 = tempRound[0U];
      uint64_t uu____1;
      Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t11, t21);
      uu____1 = Hacl_Impl_LowLevel_add8(tempRound, t21, t31);
      Hacl_Impl_LowLevel_shift8(t31, round2);
      {
        uint64_t tempRound0[8U] = { 0U };
        uint64_t t2[8U] = { 0U };
        uint64_t t32[8U] = { 0U };
        uint64_t t12 = round2[0U];
        uint64_t uu____2;
        Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t12, t2);
        uu____2 = Hacl_Impl_LowLevel_add8(round2, t2, t32);
        Hacl_Impl_LowLevel_shift8(t32, tempRound0);
        {
          uint64_t t22[8U] = { 0U };
          uint64_t t3[8U] = { 0U };
          uint64_t t1 = tempRound0[0U];
          uint64_t uu____3;
          Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t1, t22);
          uu____3 = Hacl_Impl_LowLevel_add8(tempRound0, t22, t3);
          Hacl_Impl_LowLevel_shift8(t3, round4);
          {
            uint64_t tempBuffer[4U] = { 0U };
            uint64_t tempBufferForSubborrow = (uint64_t)0U;
            uint64_t cin = round4[4U];
            uint64_t *x_ = round4;
            uint64_t
            c = Hacl_Impl_LowLevel_sub4_il(x_, Hacl_Impl_P256_LowLevel_prime256_buffer, tempBuffer);
            uint64_t
            carry = Hacl_Impl_LowLevel_sub_borrow(c, cin, (uint64_t)0U, &tempBufferForSubborrow);
            Hacl_Impl_LowLevel_cmovznz4(carry, tempBuffer, x_, result);
          }
        }
      }
    }
  }
}

static void Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowN(uint32_t n1, uint64_t *a)
{
  uint32_t i;
  for (i = (uint32_t)0U; i < n1; i = i + (uint32_t)1U)
  {
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(a, a, a);
  }
}

static void
Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowNminusOne(
  uint32_t n1,
  uint64_t *a,
  uint64_t *b
)
{
  uint32_t i;
  b[0U] = (uint64_t)1U;
  b[1U] = (uint64_t)18446744069414584320U;
  b[2U] = (uint64_t)18446744073709551615U;
  b[3U] = (uint64_t)4294967294U;
  for (i = (uint32_t)0U; i < n1; i = i + (uint32_t)1U)
  {
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(b, a, b);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(a, a, a);
  }
}

static void
Hacl_Impl_P256_MontgomeryMultiplication_exponent(
  uint64_t *a,
  uint64_t *result,
  uint64_t *tempBuffer
)
{
  uint64_t *buffer_norm_1 = tempBuffer;
  uint64_t *buffer_result1 = tempBuffer + (uint32_t)4U;
  uint64_t *buffer_result2 = tempBuffer + (uint32_t)8U;
  uint64_t *buffer_norm_3 = tempBuffer + (uint32_t)12U;
  uint64_t *buffer_result3 = tempBuffer + (uint32_t)16U;
  uint64_t *buffer_a0;
  uint64_t *buffer_b0;
  uint64_t *buffer_a;
  uint64_t *buffer_b;
  memcpy(buffer_norm_1, a, (uint32_t)4U * sizeof a[0U]);
  buffer_a0 = buffer_norm_1;
  buffer_b0 = buffer_norm_1 + (uint32_t)4U;
  Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowNminusOne((uint32_t)32U,
    buffer_a0,
    buffer_b0);
  Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)224U, buffer_b0);
  memcpy(buffer_result2, a, (uint32_t)4U * sizeof a[0U]);
  Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)192U, buffer_result2);
  memcpy(buffer_norm_3, a, (uint32_t)4U * sizeof a[0U]);
  buffer_a = buffer_norm_3;
  buffer_b = buffer_norm_3 + (uint32_t)4U;
  Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowNminusOne((uint32_t)94U, buffer_a, buffer_b);
  Hacl_Impl_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)2U, buffer_b);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    buffer_result2,
    buffer_result1);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    buffer_result3,
    buffer_result1);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    a,
    buffer_result1);
  memcpy(result, buffer_result1, (uint32_t)4U * sizeof buffer_result1[0U]);
}

void pointToDomain(uint64_t *p, uint64_t *result)
{
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *r_x = result;
  uint64_t *r_y = result + (uint32_t)4U;
  uint64_t *r_z = result + (uint32_t)8U;
  uint64_t multBuffer[8U] = { 0U };
  Hacl_Impl_LowLevel_shift_256_impl(p_x, multBuffer);
  Hacl_Impl_SolinasReduction_solinas_reduction_impl(multBuffer, r_x);
  {
    uint64_t multBuffer0[8U] = { 0U };
    Hacl_Impl_LowLevel_shift_256_impl(p_y, multBuffer0);
    Hacl_Impl_SolinasReduction_solinas_reduction_impl(multBuffer0, r_y);
    {
      uint64_t multBuffer1[8U] = { 0U };
      Hacl_Impl_LowLevel_shift_256_impl(p_z, multBuffer1);
      Hacl_Impl_SolinasReduction_solinas_reduction_impl(multBuffer1, r_z);
    }
  }
}

static void fromDomain(uint64_t *f, uint64_t *result)
{
  uint64_t t[8U] = { 0U };
  uint64_t *t_low = t;
  uint64_t round2[8U] = { 0U };
  uint64_t round4[8U] = { 0U };
  memcpy(t_low, f, (uint32_t)4U * sizeof f[0U]);
  {
    uint64_t tempRound[8U] = { 0U };
    uint64_t t20[8U] = { 0U };
    uint64_t t30[8U] = { 0U };
    uint64_t t10 = t[0U];
    uint64_t uu____0;
    Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t10, t20);
    uu____0 = Hacl_Impl_LowLevel_add8(t, t20, t30);
    Hacl_Impl_LowLevel_shift8(t30, tempRound);
    {
      uint64_t t21[8U] = { 0U };
      uint64_t t31[8U] = { 0U };
      uint64_t t11 = tempRound[0U];
      uint64_t uu____1;
      Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t11, t21);
      uu____1 = Hacl_Impl_LowLevel_add8(tempRound, t21, t31);
      Hacl_Impl_LowLevel_shift8(t31, round2);
      {
        uint64_t tempRound0[8U] = { 0U };
        uint64_t t2[8U] = { 0U };
        uint64_t t32[8U] = { 0U };
        uint64_t t12 = round2[0U];
        uint64_t uu____2;
        Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t12, t2);
        uu____2 = Hacl_Impl_LowLevel_add8(round2, t2, t32);
        Hacl_Impl_LowLevel_shift8(t32, tempRound0);
        {
          uint64_t t22[8U] = { 0U };
          uint64_t t3[8U] = { 0U };
          uint64_t t1 = tempRound0[0U];
          uint64_t uu____3;
          Hacl_Impl_LowLevel_shortened_mul(Hacl_Impl_P256_LowLevel_prime256_buffer, t1, t22);
          uu____3 = Hacl_Impl_LowLevel_add8(tempRound0, t22, t3);
          Hacl_Impl_LowLevel_shift8(t3, round4);
          {
            uint64_t tempBuffer[4U] = { 0U };
            uint64_t tempBufferForSubborrow = (uint64_t)0U;
            uint64_t cin = round4[4U];
            uint64_t *x_ = round4;
            uint64_t
            c = Hacl_Impl_LowLevel_sub4_il(x_, Hacl_Impl_P256_LowLevel_prime256_buffer, tempBuffer);
            uint64_t
            carry = Hacl_Impl_LowLevel_sub_borrow(c, cin, (uint64_t)0U, &tempBufferForSubborrow);
            Hacl_Impl_LowLevel_cmovznz4(carry, tempBuffer, x_, result);
          }
        }
      }
    }
  }
}

void pointFromDomain(uint64_t *p, uint64_t *result)
{
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *r_x = result;
  uint64_t *r_y = result + (uint32_t)4U;
  uint64_t *r_z = result + (uint32_t)8U;
  fromDomain(p_x, r_x);
  fromDomain(p_y, r_y);
  fromDomain(p_z, r_z);
}

static void quatre(uint64_t *a, uint64_t *result)
{
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(a, a, result);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(result,
    result,
    result);
}

static void multByTwo(uint64_t *a, uint64_t *out)
{
  Hacl_Impl_P256_LowLevel_p256_add(a, a, out);
}

static void multByThree(uint64_t *a, uint64_t *result)
{
  multByTwo(a, result);
  Hacl_Impl_P256_LowLevel_p256_add(a, result, result);
}

static void multByFour(uint64_t *a, uint64_t *result)
{
  multByTwo(a, result);
  multByTwo(result, result);
}

static void multByEight(uint64_t *a, uint64_t *result)
{
  multByTwo(a, result);
  multByTwo(result, result);
  multByTwo(result, result);
}

static void multByMinusThree(uint64_t *a, uint64_t *result)
{
  multByThree(a, result);
  {
    uint64_t zeros1[4U] = { 0U };
    Hacl_Impl_P256_LowLevel_p256_sub(zeros1, result, result);
  }
}

static uint64_t isZero_uint64(uint64_t *f)
{
  uint64_t a0 = f[0U];
  uint64_t a1 = f[1U];
  uint64_t a2 = f[2U];
  uint64_t a3 = f[3U];
  uint64_t r0 = FStar_UInt64_eq_mask(a0, (uint64_t)0U);
  uint64_t r1 = FStar_UInt64_eq_mask(a1, (uint64_t)0U);
  uint64_t r2 = FStar_UInt64_eq_mask(a2, (uint64_t)0U);
  uint64_t r3 = FStar_UInt64_eq_mask(a3, (uint64_t)0U);
  uint64_t r01 = r0 & r1;
  uint64_t r23 = r2 & r3;
  return r01 & r23;
}

static void copy_point(uint64_t *p, uint64_t *result)
{
  memcpy(result, p, (uint32_t)12U * sizeof p[0U]);
}

void point_double(uint64_t *p, uint64_t *result, uint64_t *tempBuffer)
{
  uint64_t *s1 = tempBuffer;
  uint64_t *m = tempBuffer + (uint32_t)4U;
  uint64_t *buffer_for_s_m = tempBuffer + (uint32_t)8U;
  uint64_t *buffer_for_x3 = tempBuffer + (uint32_t)32U;
  uint64_t *buffer_for_y3 = tempBuffer + (uint32_t)40U;
  uint64_t *pypz = tempBuffer + (uint32_t)56U;
  uint64_t *x3 = tempBuffer + (uint32_t)60U;
  uint64_t *y3 = tempBuffer + (uint32_t)64U;
  uint64_t *z3 = tempBuffer + (uint32_t)68U;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *px = p;
  uint64_t *py = p + (uint32_t)4U;
  uint64_t *pz = p + (uint32_t)8U;
  uint64_t *yy = buffer_for_s_m;
  uint64_t *xyy = buffer_for_s_m + (uint32_t)4U;
  uint64_t *zzzz = buffer_for_s_m + (uint32_t)8U;
  uint64_t *minThreeZzzz = buffer_for_s_m + (uint32_t)12U;
  uint64_t *xx = buffer_for_s_m + (uint32_t)16U;
  uint64_t *threeXx = buffer_for_s_m + (uint32_t)20U;
  uint64_t *twoS;
  uint64_t *mm;
  uint64_t *yyyy;
  uint64_t *eightYyyy;
  uint64_t *sx3;
  uint64_t *msx3;
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(py, py, yy);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(px, yy, xyy);
  multByFour(xyy, s1);
  quatre(pz, zzzz);
  multByMinusThree(zzzz, minThreeZzzz);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(px, px, xx);
  multByThree(xx, threeXx);
  Hacl_Impl_P256_LowLevel_p256_add(minThreeZzzz, threeXx, m);
  twoS = buffer_for_x3;
  mm = buffer_for_x3 + (uint32_t)4U;
  multByTwo(s1, twoS);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(m, m, mm);
  Hacl_Impl_P256_LowLevel_p256_sub(mm, twoS, x3);
  yyyy = buffer_for_y3;
  eightYyyy = buffer_for_y3 + (uint32_t)4U;
  sx3 = buffer_for_y3 + (uint32_t)8U;
  msx3 = buffer_for_y3 + (uint32_t)12U;
  quatre(p_y, yyyy);
  multByEight(yyyy, eightYyyy);
  Hacl_Impl_P256_LowLevel_p256_sub(s1, x3, sx3);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(m, sx3, msx3);
  Hacl_Impl_P256_LowLevel_p256_sub(msx3, eightYyyy, y3);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(p_y, p_z, pypz);
  multByTwo(pypz, z3);
  memcpy(result, x3, (uint32_t)4U * sizeof x3[0U]);
  memcpy(result + (uint32_t)4U, y3, (uint32_t)4U * sizeof y3[0U]);
  memcpy(result + (uint32_t)8U, z3, (uint32_t)4U * sizeof z3[0U]);
}

static void copy_conditional(uint64_t *out, uint64_t *x, uint64_t mask)
{
  uint64_t out_0 = out[0U];
  uint64_t out_1 = out[1U];
  uint64_t out_2 = out[2U];
  uint64_t out_3 = out[3U];
  uint64_t x_0 = x[0U];
  uint64_t x_1 = x[1U];
  uint64_t x_2 = x[2U];
  uint64_t x_3 = x[3U];
  uint64_t out_01 = out_0;
  uint64_t out_11 = out_1;
  uint64_t out_21 = out_2;
  uint64_t out_31 = out_3;
  uint64_t x_01 = x_0;
  uint64_t x_11 = x_1;
  uint64_t x_21 = x_2;
  uint64_t x_31 = x_3;
  uint64_t r_0 = out_01 ^ (mask & (out_01 ^ x_01));
  uint64_t r_1 = out_11 ^ (mask & (out_11 ^ x_11));
  uint64_t r_2 = out_21 ^ (mask & (out_21 ^ x_21));
  uint64_t r_3 = out_31 ^ (mask & (out_31 ^ x_31));
  uint64_t temp_0 = r_0;
  uint64_t temp_1 = r_1;
  uint64_t temp_2 = r_2;
  uint64_t temp_3 = r_3;
  out[0U] = temp_0;
  out[1U] = temp_1;
  out[2U] = temp_2;
  out[3U] = temp_3;
}

static void
copy_point_conditional(
  uint64_t *x3_out,
  uint64_t *y3_out,
  uint64_t *z3_out,
  uint64_t *p,
  uint64_t *maskPoint
)
{
  uint64_t *z = maskPoint + (uint32_t)8U;
  uint64_t mask = isZero_uint64(z);
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  copy_conditional(x3_out, p_x, mask);
  copy_conditional(y3_out, p_y, mask);
  copy_conditional(z3_out, p_z, mask);
}

uint64_t compare_felem(uint64_t *a, uint64_t *b)
{
  uint64_t a_0 = a[0U];
  uint64_t a_1 = a[1U];
  uint64_t a_2 = a[2U];
  uint64_t a_3 = a[3U];
  uint64_t b_0 = b[0U];
  uint64_t b_1 = b[1U];
  uint64_t b_2 = b[2U];
  uint64_t b_3 = b[3U];
  uint64_t r_0 = FStar_UInt64_eq_mask(a_0, b_0);
  uint64_t r_1 = FStar_UInt64_eq_mask(a_1, b_1);
  uint64_t r_2 = FStar_UInt64_eq_mask(a_2, b_2);
  uint64_t r_3 = FStar_UInt64_eq_mask(a_3, b_3);
  uint64_t r01 = r_0 & r_1;
  uint64_t r23 = r_2 & r_3;
  return r01 & r23;
}

void point_add(uint64_t *p, uint64_t *q, uint64_t *result, uint64_t *tempBuffer)
{
  uint64_t *z1 = p + (uint32_t)8U;
  uint64_t *z2 = q + (uint32_t)8U;
  uint64_t *tempBuffer16 = tempBuffer;
  uint64_t *u11 = tempBuffer + (uint32_t)16U;
  uint64_t *u2 = tempBuffer + (uint32_t)20U;
  uint64_t *s1 = tempBuffer + (uint32_t)24U;
  uint64_t *s2 = tempBuffer + (uint32_t)28U;
  uint64_t *h = tempBuffer + (uint32_t)32U;
  uint64_t *r = tempBuffer + (uint32_t)36U;
  uint64_t *uh = tempBuffer + (uint32_t)40U;
  uint64_t *hCube = tempBuffer + (uint32_t)44U;
  uint64_t *tempBuffer28 = tempBuffer + (uint32_t)60U;
  uint64_t *x1 = p;
  uint64_t *y1 = p + (uint32_t)4U;
  uint64_t *z110 = p + (uint32_t)8U;
  uint64_t *x2 = q;
  uint64_t *y2 = q + (uint32_t)4U;
  uint64_t *z210 = q + (uint32_t)8U;
  uint64_t *z2Square = tempBuffer16;
  uint64_t *z1Square = tempBuffer16 + (uint32_t)4U;
  uint64_t *z2Cube = tempBuffer16 + (uint32_t)8U;
  uint64_t *z1Cube = tempBuffer16 + (uint32_t)12U;
  uint64_t one1;
  uint64_t two;
  uint64_t z1notZero;
  uint64_t z2notZero;
  uint64_t pointsInf;
  uint64_t onetwo;
  uint64_t result1;
  bool flag;
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z210, z210, z2Square);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z110, z110, z1Square);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z2Square,
    z210,
    z2Cube);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z1Square,
    z110,
    z1Cube);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(x1, z2Square, u11);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(x2, z1Square, u2);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(y1, z2Cube, s1);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(y2, z1Cube, s2);
  one1 = compare_felem(u11, u2);
  two = compare_felem(s1, s2);
  z1notZero = isZero_uint64(z1);
  z2notZero = isZero_uint64(z2);
  pointsInf = ~z1notZero & ~z2notZero;
  onetwo = one1 & two;
  result1 = onetwo & pointsInf;
  flag = result1 == (uint64_t)0xffffffffffffffffU;
  if (flag)
  {
    point_double(p, result, tempBuffer);
  }
  else
  {
    uint64_t *temp = tempBuffer16;
    uint64_t *z11;
    uint64_t *z21;
    uint64_t *tempBuffer161;
    uint64_t *x3_out1;
    uint64_t *y3_out1;
    uint64_t *z3_out1;
    uint64_t *rSquare;
    uint64_t *r_h;
    uint64_t *twouh;
    uint64_t *s1hCube;
    uint64_t *u1hx3;
    uint64_t *ru1hx3;
    uint64_t *z1z2;
    Hacl_Impl_P256_LowLevel_p256_sub(u2, u11, h);
    Hacl_Impl_P256_LowLevel_p256_sub(s2, s1, r);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(h, h, temp);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(u11, temp, uh);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(h, temp, hCube);
    z11 = p + (uint32_t)8U;
    z21 = q + (uint32_t)8U;
    tempBuffer161 = tempBuffer28;
    x3_out1 = tempBuffer28 + (uint32_t)16U;
    y3_out1 = tempBuffer28 + (uint32_t)20U;
    z3_out1 = tempBuffer28 + (uint32_t)24U;
    rSquare = tempBuffer161;
    r_h = tempBuffer161 + (uint32_t)4U;
    twouh = tempBuffer161 + (uint32_t)8U;
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(r, r, rSquare);
    Hacl_Impl_P256_LowLevel_p256_sub(rSquare, hCube, r_h);
    multByTwo(uh, twouh);
    Hacl_Impl_P256_LowLevel_p256_sub(r_h, twouh, x3_out1);
    s1hCube = tempBuffer161;
    u1hx3 = tempBuffer161 + (uint32_t)4U;
    ru1hx3 = tempBuffer161 + (uint32_t)8U;
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(s1, hCube, s1hCube);
    Hacl_Impl_P256_LowLevel_p256_sub(uh, x3_out1, u1hx3);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(r, u1hx3, ru1hx3);
    Hacl_Impl_P256_LowLevel_p256_sub(ru1hx3, s1hCube, y3_out1);
    z1z2 = tempBuffer161;
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z11, z21, z1z2);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(h, z1z2, z3_out1);
    copy_point_conditional(x3_out1, y3_out1, z3_out1, q, p);
    copy_point_conditional(x3_out1, y3_out1, z3_out1, p, q);
    memcpy(result, x3_out1, (uint32_t)4U * sizeof x3_out1[0U]);
    memcpy(result + (uint32_t)4U, y3_out1, (uint32_t)4U * sizeof y3_out1[0U]);
    memcpy(result + (uint32_t)8U, z3_out1, (uint32_t)4U * sizeof z3_out1[0U]);
  }
}

uint64_t isPointAtInfinityPrivate(uint64_t *p)
{
  uint64_t z0 = p[8U];
  uint64_t z1 = p[9U];
  uint64_t z2 = p[10U];
  uint64_t z3 = p[11U];
  uint64_t z0_zero = FStar_UInt64_eq_mask(z0, (uint64_t)0U);
  uint64_t z1_zero = FStar_UInt64_eq_mask(z1, (uint64_t)0U);
  uint64_t z2_zero = FStar_UInt64_eq_mask(z2, (uint64_t)0U);
  uint64_t z3_zero = FStar_UInt64_eq_mask(z3, (uint64_t)0U);
  return (z0_zero & z1_zero) & (z2_zero & z3_zero);
}

void norm(uint64_t *p, uint64_t *resultPoint, uint64_t *tempBuffer)
{
  uint64_t *xf = p;
  uint64_t *yf = p + (uint32_t)4U;
  uint64_t *zf = p + (uint32_t)8U;
  uint64_t *z2f = tempBuffer + (uint32_t)4U;
  uint64_t *z3f = tempBuffer + (uint32_t)8U;
  uint64_t *tempBuffer20 = tempBuffer + (uint32_t)12U;
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(zf, zf, z2f);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z2f, zf, z3f);
  Hacl_Impl_P256_MontgomeryMultiplication_exponent(z2f, z2f, tempBuffer20);
  Hacl_Impl_P256_MontgomeryMultiplication_exponent(z3f, z3f, tempBuffer20);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(xf, z2f, z2f);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(yf, z3f, z3f);
  {
    uint64_t zeroBuffer[4U] = { 0U };
    uint64_t *resultX = resultPoint;
    uint64_t *resultY = resultPoint + (uint32_t)4U;
    uint64_t *resultZ = resultPoint + (uint32_t)8U;
    uint64_t bit = isPointAtInfinityPrivate(p);
    fromDomain(z2f, resultX);
    fromDomain(z3f, resultY);
    resultZ[0U] = (uint64_t)1U;
    resultZ[1U] = (uint64_t)0U;
    resultZ[2U] = (uint64_t)0U;
    resultZ[3U] = (uint64_t)0U;
    copy_conditional(resultZ, zeroBuffer, bit);
  }
}

static void zero_buffer(uint64_t *p)
{
  p[0U] = (uint64_t)0U;
  p[1U] = (uint64_t)0U;
  p[2U] = (uint64_t)0U;
  p[3U] = (uint64_t)0U;
  p[4U] = (uint64_t)0U;
  p[5U] = (uint64_t)0U;
  p[6U] = (uint64_t)0U;
  p[7U] = (uint64_t)0U;
  p[8U] = (uint64_t)0U;
  p[9U] = (uint64_t)0U;
  p[10U] = (uint64_t)0U;
  p[11U] = (uint64_t)0U;
}

void
scalarMultiplicationI(uint64_t *p, uint64_t *result, uint8_t *scalar, uint64_t *tempBuffer)
{
  uint64_t *q = tempBuffer;
  uint64_t *buff;
  zero_buffer(q);
  buff = tempBuffer + (uint32_t)12U;
  pointToDomain(p, result);
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)256U; i = i + (uint32_t)1U)
    {
      uint32_t bit0 = (uint32_t)255U - i;
      uint64_t bit = (uint64_t)(scalar[bit0 / (uint32_t)8U] >> bit0 % (uint32_t)8U & (uint8_t)1U);
      Hacl_Spec_P256_Ladder_cswap(bit, q, result);
      point_add(q, result, result, buff);
      point_double(q, q, buff);
      Hacl_Spec_P256_Ladder_cswap(bit, q, result);
    }
  }
  norm(q, result, buff);
}

static void uploadBasePoint(uint64_t *p)
{
  p[0U] = (uint64_t)8784043285714375740U;
  p[1U] = (uint64_t)8483257759279461889U;
  p[2U] = (uint64_t)8789745728267363600U;
  p[3U] = (uint64_t)1770019616739251654U;
  p[4U] = (uint64_t)15992936863339206154U;
  p[5U] = (uint64_t)10037038012062884956U;
  p[6U] = (uint64_t)15197544864945402661U;
  p[7U] = (uint64_t)9615747158586711429U;
  p[8U] = (uint64_t)1U;
  p[9U] = (uint64_t)18446744069414584320U;
  p[10U] = (uint64_t)18446744073709551615U;
  p[11U] = (uint64_t)4294967294U;
}

void
scalarMultiplicationWithoutNorm(
  uint64_t *p,
  uint64_t *result,
  uint8_t *scalar,
  uint64_t *tempBuffer
)
{
  uint64_t *q = tempBuffer;
  uint64_t *buff;
  zero_buffer(q);
  buff = tempBuffer + (uint32_t)12U;
  pointToDomain(p, result);
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)256U; i = i + (uint32_t)1U)
    {
      uint32_t bit0 = (uint32_t)255U - i;
      uint64_t bit = (uint64_t)(scalar[bit0 / (uint32_t)8U] >> bit0 % (uint32_t)8U & (uint8_t)1U);
      Hacl_Spec_P256_Ladder_cswap(bit, q, result);
      point_add(q, result, result, buff);
      point_double(q, q, buff);
      Hacl_Spec_P256_Ladder_cswap(bit, q, result);
    }
  }
  copy_point(q, result);
}

void secretToPublic(uint64_t *result, uint8_t *scalar, uint64_t *tempBuffer)
{
  uint64_t basePoint[12U] = { 0U };
  uint64_t *q;
  uint64_t *buff;
  uploadBasePoint(basePoint);
  q = tempBuffer;
  buff = tempBuffer + (uint32_t)12U;
  zero_buffer(q);
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)256U; i = i + (uint32_t)1U)
    {
      uint32_t bit0 = (uint32_t)255U - i;
      uint64_t bit = (uint64_t)(scalar[bit0 / (uint32_t)8U] >> bit0 % (uint32_t)8U & (uint8_t)1U);
      Hacl_Spec_P256_Ladder_cswap(bit, q, basePoint);
      point_add(q, basePoint, basePoint, buff);
      point_double(q, q, buff);
      Hacl_Spec_P256_Ladder_cswap(bit, q, basePoint);
    }
  }
  norm(q, result, buff);
}

void secretToPublicWithoutNorm(uint64_t *result, uint8_t *scalar, uint64_t *tempBuffer)
{
  uint64_t basePoint[12U] = { 0U };
  uint64_t *q;
  uint64_t *buff;
  uploadBasePoint(basePoint);
  q = tempBuffer;
  buff = tempBuffer + (uint32_t)12U;
  zero_buffer(q);
  {
    uint32_t i;
    for (i = (uint32_t)0U; i < (uint32_t)256U; i = i + (uint32_t)1U)
    {
      uint32_t bit0 = (uint32_t)255U - i;
      uint64_t bit = (uint64_t)(scalar[bit0 / (uint32_t)8U] >> bit0 % (uint32_t)8U & (uint8_t)1U);
      Hacl_Spec_P256_Ladder_cswap(bit, q, basePoint);
      point_add(q, basePoint, basePoint, buff);
      point_double(q, q, buff);
      Hacl_Spec_P256_Ladder_cswap(bit, q, basePoint);
    }
  }
  copy_point(q, result);
}

bool isPointAtInfinity(uint64_t *p)
{
  uint64_t z0 = p[8U];
  uint64_t z1 = p[9U];
  uint64_t z2 = p[10U];
  uint64_t z3 = p[11U];
  bool z0_zero = z0 == (uint64_t)0U;
  bool z1_zero = z1 == (uint64_t)0U;
  bool z2_zero = z2 == (uint64_t)0U;
  bool z3_zero = z3 == (uint64_t)0U;
  return z0_zero && z1_zero && z2_zero && z3_zero;
}

bool isPointOnCurve(uint64_t *p)
{
  uint64_t y2Buffer[4U] = { 0U };
  uint64_t xBuffer[4U] = { 0U };
  uint64_t *x = p;
  uint64_t *y = p + (uint32_t)4U;
  uint64_t multBuffer0[8U] = { 0U };
  Hacl_Impl_LowLevel_shift_256_impl(y, multBuffer0);
  Hacl_Impl_SolinasReduction_solinas_reduction_impl(multBuffer0, y2Buffer);
  Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(y2Buffer,
    y2Buffer,
    y2Buffer);
  {
    uint64_t xToDomainBuffer[4U] = { 0U };
    uint64_t minusThreeXBuffer[4U] = { 0U };
    uint64_t p256_constant[4U] = { 0U };
    uint64_t multBuffer[8U] = { 0U };
    uint64_t r;
    bool z1;
    Hacl_Impl_LowLevel_shift_256_impl(x, multBuffer);
    Hacl_Impl_SolinasReduction_solinas_reduction_impl(multBuffer, xToDomainBuffer);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(xToDomainBuffer,
      xToDomainBuffer,
      xBuffer);
    Hacl_Impl_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(xBuffer,
      xToDomainBuffer,
      xBuffer);
    multByThree(xToDomainBuffer, minusThreeXBuffer);
    Hacl_Impl_P256_LowLevel_p256_sub(xBuffer, minusThreeXBuffer, xBuffer);
    p256_constant[0U] = (uint64_t)15608596021259845087U;
    p256_constant[1U] = (uint64_t)12461466548982526096U;
    p256_constant[2U] = (uint64_t)16546823903870267094U;
    p256_constant[3U] = (uint64_t)15866188208926050356U;
    Hacl_Impl_P256_LowLevel_p256_add(xBuffer, p256_constant, xBuffer);
    r = compare_felem(y2Buffer, xBuffer);
    z1 = !(r == (uint64_t)0U);
    return z1;
  }
}

