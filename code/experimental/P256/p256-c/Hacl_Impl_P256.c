/* 
  This file was generated by KreMLin <https://github.com/FStarLang/kremlin>
  KreMLin invocation: /home/nkulatov/new/kremlin/krml -fbuiltin-uint128 -funroll-loops 8 -add-include "TestLib.h" /dist/generic/testlib.c -skip-compilation -no-prefix Hacl.Impl.P256 -no-prefix Hacl.Impl.SolinasReduction -bundle Lib.* -bundle Spec.* -bundle Hacl.Impl.P256=Hacl.Impl.*,Hacl.Spec.P256.*,Hacl.Spec.Curve25519.*,Hacl.Impl.Curve25519.* -library C,FStar -drop LowStar,Spec,Prims,Lib,C.Loops.*,Hacl.Spec.P256.Lemmas -add-include "c/Lib_PrintBuffer.h" -add-include "FStar_UInt_8_16_32_64.h" -tmpdir p256-c .output/prims.krml .output/FStar_Pervasives_Native.krml .output/FStar_Pervasives.krml .output/FStar_Reflection_Types.krml .output/FStar_Reflection_Data.krml .output/FStar_Order.krml .output/FStar_Reflection_Basic.krml .output/FStar_Mul.krml .output/FStar_Preorder.krml .output/FStar_Calc.krml .output/FStar_Squash.krml .output/FStar_Classical.krml .output/FStar_StrongExcludedMiddle.krml .output/FStar_FunctionalExtensionality.krml .output/FStar_List_Tot_Base.krml .output/FStar_List_Tot_Properties.krml .output/FStar_List_Tot.krml .output/FStar_Seq_Base.krml .output/FStar_Seq_Properties.krml .output/FStar_Seq.krml .output/FStar_Math_Lib.krml .output/FStar_Math_Lemmas.krml .output/FStar_BitVector.krml .output/FStar_UInt.krml .output/FStar_UInt32.krml .output/FStar_Int.krml .output/FStar_Int16.krml .output/FStar_Ghost.krml .output/FStar_ErasedLogic.krml .output/FStar_UInt64.krml .output/FStar_Set.krml .output/FStar_PropositionalExtensionality.krml .output/FStar_PredicateExtensionality.krml .output/FStar_TSet.krml .output/FStar_Monotonic_Heap.krml .output/FStar_Heap.krml .output/FStar_Map.krml .output/FStar_Monotonic_HyperHeap.krml .output/FStar_Monotonic_HyperStack.krml .output/FStar_HyperStack.krml .output/FStar_Monotonic_Witnessed.krml .output/FStar_HyperStack_ST.krml .output/FStar_HyperStack_All.krml .output/FStar_Exn.krml .output/FStar_ST.krml .output/FStar_All.krml .output/FStar_List.krml .output/FStar_Reflection_Const.krml .output/FStar_Char.krml .output/FStar_String.krml .output/FStar_Reflection_Derived.krml .output/FStar_Reflection_Derived_Lemmas.krml .output/FStar_Reflection.krml .output/FStar_Range.krml .output/FStar_Tactics_Types.krml .output/FStar_Tactics_Result.krml .output/FStar_Tactics_Effect.krml .output/FStar_Tactics_Util.krml .output/FStar_Tactics_Builtins.krml .output/FStar_Reflection_Formula.krml .output/FStar_Tactics_Derived.krml .output/FStar_Tactics_Logic.krml .output/FStar_Tactics.krml .output/FStar_Reflection_Arith.krml .output/FStar_Tactics_Canon.krml .output/FStar_Int64.krml .output/FStar_Int63.krml .output/FStar_Int32.krml .output/FStar_Int8.krml .output/FStar_UInt63.krml .output/FStar_UInt16.krml .output/FStar_UInt8.krml .output/FStar_Int_Cast.krml .output/FStar_UInt128.krml .output/FStar_Int_Cast_Full.krml .output/Lib_IntTypes.krml .output/Lib_RawIntTypes.krml .output/Lib_LoopCombinators.krml .output/Spec_Curve25519_Lemmas.krml .output/Lib_Sequence.krml .output/Lib_ByteSequence.krml .output/Spec_Curve25519.krml .output/Hacl_Spec_Curve25519_Field64_Definition.krml .output/FStar_Universe.krml .output/FStar_GSet.krml .output/FStar_ModifiesGen.krml .output/FStar_BigOps.krml .output/LowStar_Monotonic_Buffer.krml .output/LowStar_Buffer.krml .output/LowStar_BufferOps.krml .output/Spec_Loops.krml .output/C_Loops.krml .output/Lib_Loops.krml .output/LowStar_ImmutableBuffer.krml .output/Lib_Buffer.krml .output/Hacl_Spec_P256_Definitions.krml .output/Hacl_Impl_Curve25519_Lemmas.krml .output/Hacl_Spec_Curve25519_Field64_Lemmas.krml .output/Hacl_Spec_Curve25519_Field64_Core.krml .output/Hacl_Spec_P256_Lemmas.krml .output/Hacl_Spec_P256_Core.krml .output/Hacl_Spec_P256_SolinasReduction.krml .output/Hacl_Impl_Curve25519_Field64_Core.krml .output/Hacl_Impl_Gen.krml .output/Hacl_Spec_P256_MontgomeryMultiplication.krml .output/Hacl_Spec_P256_MontgomeryMultiplication_PointDouble.krml .output/Hacl_Spec_P256_MontgomeryMultiplication_PointAdd.krml .output/Hacl_Spec_P256_Ladder.krml .output/Hacl_Impl_SolinasReduction.krml .output/Hacl_Spec_P256_Normalisation.krml .output/Hacl_Impl_P256.krml
  F* version: f2134fe1
  KreMLin version: f534ac02
 */

#include "Hacl_Impl_P256.h"

static uint64_t Hacl_Spec_P256_Core_store_high_low_u(uint32_t high, uint32_t low)
{
  uint64_t as_uint64_high = (uint64_t)high;
  uint64_t as_uint64_high1 = as_uint64_high << (uint32_t)32U;
  uint64_t as_uint64_low = (uint64_t)low;
  return as_uint64_low ^ as_uint64_high1;
}

static uint64_t
Hacl_Impl_Gen_prime_buffer[4U] =
  {
    (uint64_t)0xffffffffffffffffU,
    (uint64_t)0xffffffffU,
    (uint64_t)0U,
    (uint64_t)0xffffffff00000001U
  };

static uint64_t
Hacl_Impl_Gen_add_carry(uint64_t cin, uint64_t x, uint64_t y, uint64_t *result1)
{
  uint64_t res1 = x + cin;
  uint64_t c;
  if (res1 < cin)
    c = (uint64_t)1U;
  else
    c = (uint64_t)0U;
  uint64_t res = res1 + y;
  uint64_t c1;
  if (res < res1)
    c1 = c + (uint64_t)1U;
  else
    c1 = c;
  result1[0U] = res;
  return c1;
}

static uint64_t
Hacl_Impl_Gen_sub_borrow(uint64_t cin, uint64_t x, uint64_t y, uint64_t *result1)
{
  uint64_t res = x - y - cin;
  uint64_t c;
  if (cin == (uint64_t)1U)
  {
    uint64_t ite;
    if (x <= y)
      ite = (uint64_t)1U;
    else
      ite = (uint64_t)0U;
    c = ite;
  }
  else
  {
    uint64_t ite;
    if (x < y)
      ite = (uint64_t)1U;
    else
      ite = (uint64_t)0U;
    c = ite;
  }
  result1[0U] = res;
  return c;
}

static uint64_t Hacl_Impl_Gen_sub4(uint64_t *x, uint64_t *y, uint64_t *result)
{
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, x[0U], y[0U], r0);
  uint64_t cc1 = Hacl_Impl_Gen_sub_borrow(cc, x[1U], y[1U], r1);
  uint64_t cc2 = Hacl_Impl_Gen_sub_borrow(cc1, x[2U], y[2U], r2);
  uint64_t cc3 = Hacl_Impl_Gen_sub_borrow(cc2, x[3U], y[3U], r3);
  return cc3;
}

static void Hacl_Impl_Gen_cmovznz4(uint64_t cin, uint64_t *x, uint64_t *y, uint64_t *r)
{
  uint64_t mask = ~FStar_UInt64_eq_mask(cin, (uint64_t)0U);
  uint64_t r0 = y[0U] & mask | x[0U] & ~mask;
  uint64_t r1 = y[1U] & mask | x[1U] & ~mask;
  uint64_t r2 = y[2U] & mask | x[2U] & ~mask;
  uint64_t r3 = y[3U] & mask | x[3U] & ~mask;
  r[0U] = r0;
  r[1U] = r1;
  r[2U] = r2;
  r[3U] = r3;
}

static void Hacl_Impl_Gen_reduction_prime_2prime_impl(uint64_t *x, uint64_t *result)
{
  uint64_t tempBuffer[4U] = { 0U };
  uint64_t c = Hacl_Impl_Gen_sub4(x, Hacl_Impl_Gen_prime_buffer, tempBuffer);
  Hacl_Impl_Gen_cmovznz4(c, tempBuffer, x, result);
}

static void Hacl_Impl_Gen_shift_256_impl(uint64_t *i, uint64_t *o)
{
  o[0U] = (uint64_t)0U;
  o[1U] = (uint64_t)0U;
  o[2U] = (uint64_t)0U;
  o[3U] = (uint64_t)0U;
  o[4U] = i[0U];
  o[5U] = i[1U];
  o[6U] = i[2U];
  o[7U] = i[3U];
}

static void Hacl_Impl_Gen_p256_add(uint64_t *arg1, uint64_t *arg2, uint64_t *out)
{
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_Gen_add_carry((uint64_t)0U, arg1[0U], arg2[0U], r0);
  uint64_t cc1 = Hacl_Impl_Gen_add_carry(cc, arg1[1U], arg2[1U], r1);
  uint64_t cc2 = Hacl_Impl_Gen_add_carry(cc1, arg1[2U], arg2[2U], r2);
  uint64_t cc3 = Hacl_Impl_Gen_add_carry(cc2, arg1[3U], arg2[3U], r3);
  uint64_t t = cc3;
  uint64_t cc4 = Hacl_Impl_Gen_add_carry(cc3, out[0U], (uint64_t)0U, r0);
  uint64_t cc5 = Hacl_Impl_Gen_add_carry(cc4, out[1U], (uint64_t)0U - (t << (uint32_t)32U), r1);
  uint64_t cc6 = Hacl_Impl_Gen_add_carry(cc5, out[2U], (uint64_t)0U - t, r2);
  uint64_t
  uu____0 = Hacl_Impl_Gen_add_carry(cc6, out[3U], (t << (uint32_t)32U) - (t << (uint32_t)1U), r3);
}

static void Hacl_Impl_Gen_p256_double(uint64_t *arg1, uint64_t *out)
{
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_Gen_add_carry((uint64_t)0U, arg1[0U], arg1[0U], r0);
  uint64_t cc1 = Hacl_Impl_Gen_add_carry(cc, arg1[1U], arg1[1U], r1);
  uint64_t cc2 = Hacl_Impl_Gen_add_carry(cc1, arg1[2U], arg1[2U], r2);
  uint64_t cc3 = Hacl_Impl_Gen_add_carry(cc2, arg1[3U], arg1[3U], r3);
  uint64_t t = cc3;
  uint64_t cc4 = Hacl_Impl_Gen_add_carry(cc3, out[0U], (uint64_t)0U, r0);
  uint64_t cc5 = Hacl_Impl_Gen_add_carry(cc4, out[1U], (uint64_t)0U - (t << (uint32_t)32U), r1);
  uint64_t cc6 = Hacl_Impl_Gen_add_carry(cc5, out[2U], (uint64_t)0U - t, r2);
  uint64_t
  uu____0 = Hacl_Impl_Gen_add_carry(cc6, out[3U], (t << (uint32_t)32U) - (t << (uint32_t)1U), r3);
}

static void Hacl_Impl_Gen_p256_sub(uint64_t *arg1, uint64_t *arg2, uint64_t *out)
{
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, arg1[0U], arg2[0U], r0);
  uint64_t cc1 = Hacl_Impl_Gen_sub_borrow(cc, arg1[1U], arg2[1U], r1);
  uint64_t cc2 = Hacl_Impl_Gen_sub_borrow(cc1, arg1[2U], arg2[2U], r2);
  uint64_t cc3 = Hacl_Impl_Gen_sub_borrow(cc2, arg1[3U], arg2[3U], r3);
  uint64_t t = cc3;
  uint64_t cc4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, out[0U], (uint64_t)0U - t, r0);
  uint64_t cc5 = Hacl_Impl_Gen_add_carry(cc4, out[1U], (uint64_t)0U - t >> (uint32_t)32U, r1);
  uint64_t cc6 = Hacl_Impl_Gen_add_carry(cc5, out[2U], (uint64_t)0U, r2);
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(cc6, out[3U], t - (t << (uint32_t)32U), r3);
}

static uint64_t Hacl_Impl_Gen_mm_round1(uint64_t *a, uint64_t t4, uint64_t *tempBuffer)
{
  uint64_t tempBufferLocal[2U] = { 0U };
  uint64_t *temp_zl = tempBufferLocal;
  uint64_t *temp_zh = tempBufferLocal + (uint32_t)1U;
  uint64_t x = a[1U];
  uint64_t *t0_b = tempBuffer;
  uint64_t *t1_b = tempBuffer + (uint32_t)1U;
  uint64_t *t2_b = tempBuffer + (uint32_t)2U;
  uint64_t *t3_b = tempBuffer + (uint32_t)3U;
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, tempBuffer[4U], tempBuffer[0U], temp_zl);
  uint64_t f = temp_zl[0U];
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, tempBuffer[5U], (uint64_t)0U, t0_b);
  uint128_t res = (uint128_t)x * x;
  uint64_t zl = (uint64_t)res;
  uint64_t zh = (uint64_t)(res >> (uint32_t)64U);
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, tempBuffer[0U], temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, zh, (uint64_t)0U, temp_zh);
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], tempBuffer[1U], t0_b);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, temp_zh[0U], (uint64_t)0U, t1_b);
  uint128_t res0 = (uint128_t)x * a[2U];
  uint64_t zl1 = (uint64_t)res0;
  uint64_t zh1 = (uint64_t)(res0 >> (uint32_t)64U);
  tempBuffer[10U] = zl1;
  tempBuffer[11U] = zh1;
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, tempBuffer[1U], temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, zh1, (uint64_t)0U, temp_zh);
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], tempBuffer[2U], t1_b);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, temp_zh[0U], (uint64_t)0U, t2_b);
  uint128_t res1 = (uint128_t)x * a[3U];
  uint64_t zl2 = (uint64_t)res1;
  uint64_t zh2 = (uint64_t)(res1 >> (uint32_t)64U);
  tempBuffer[12U] = zl2;
  tempBuffer[13U] = zh2;
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, tempBuffer[2U], temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, zh2, (uint64_t)0U, temp_zh);
  uint64_t k6 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], tempBuffer[3U], t2_b);
  uint64_t uu____6 = Hacl_Impl_Gen_add_carry(k6, temp_zh[0U], (uint64_t)0U, t3_b);
  uint64_t t41 = Hacl_Impl_Gen_add_carry((uint64_t)0U, tempBuffer[3U], t4, t3_b);
  uint64_t k7 = Hacl_Impl_Gen_add_carry((uint64_t)0U, tempBuffer[0U], f << (uint32_t)32U, t0_b);
  uint64_t k8 = Hacl_Impl_Gen_add_carry(k7, tempBuffer[1U], f >> (uint32_t)32U, t1_b);
  uint64_t m = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, f, f << (uint32_t)32U, temp_zl);
  uint64_t uu____7 = Hacl_Impl_Gen_sub_borrow(m, f, f >> (uint32_t)32U, temp_zh);
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, tempBuffer[2U], temp_zl[0U], t2_b);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, tempBuffer[3U], temp_zh[0U], t3_b);
  uint64_t uu____8 = Hacl_Impl_Gen_add_carry(k10, t41, (uint64_t)0U, temp_zl);
  uint64_t t42 = temp_zl[0U];
  return t42;
}

static uint64_t Hacl_Impl_Gen_mm_round2(uint64_t *a, uint64_t t4, uint64_t *tempBuffer)
{
  uint64_t tempBufferLocal[2U] = { 0U };
  uint64_t *temp_zl = tempBufferLocal;
  uint64_t *temp_zh = tempBufferLocal + (uint32_t)1U;
  uint64_t x = a[2U];
  uint64_t a3 = a[3U];
  uint64_t t0 = tempBuffer[0U];
  uint64_t t1 = tempBuffer[1U];
  uint64_t t2 = tempBuffer[2U];
  uint64_t t3 = tempBuffer[3U];
  uint64_t *t0_b = tempBuffer;
  uint64_t *t1_b = tempBuffer + (uint32_t)1U;
  uint64_t *t2_b = tempBuffer + (uint32_t)2U;
  uint64_t *t3_b = tempBuffer + (uint32_t)3U;
  uint64_t zl = tempBuffer[6U];
  uint64_t zh = tempBuffer[7U];
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, t0, temp_zl);
  uint64_t f = temp_zl[0U];
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, zh, (uint64_t)0U, t0_b);
  uint64_t zl1 = tempBuffer[10U];
  uint64_t zh1 = tempBuffer[11U];
  uint64_t t01 = t0_b[0U];
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, t01, temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, zh1, (uint64_t)0U, temp_zh);
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t1, t0_b);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, temp_zh[0U], (uint64_t)0U, t1_b);
  uint128_t res = (uint128_t)x * x;
  uint64_t zl2 = (uint64_t)res;
  uint64_t zh2 = (uint64_t)(res >> (uint32_t)64U);
  uint64_t t11 = t1_b[0U];
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, t11, temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, zh2, (uint64_t)0U, temp_zh);
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t2, t1_b);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, temp_zh[0U], (uint64_t)0U, t2_b);
  uint128_t res0 = (uint128_t)x * a3;
  uint64_t zl3 = (uint64_t)res0;
  uint64_t zh3 = (uint64_t)(res0 >> (uint32_t)64U);
  tempBuffer[14U] = zl3;
  tempBuffer[15U] = zh3;
  uint64_t t21 = t2_b[0U];
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl3, t21, temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, zh3, (uint64_t)0U, temp_zh);
  uint64_t k6 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t3, t2_b);
  uint64_t uu____6 = Hacl_Impl_Gen_add_carry(k6, temp_zh[0U], (uint64_t)0U, t3_b);
  uint64_t t31 = t3_b[0U];
  uint64_t t02 = t0_b[0U];
  uint64_t t12 = t1_b[0U];
  uint64_t t41 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t31, t4, t3_b);
  uint64_t k7 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t02, f << (uint32_t)32U, t0_b);
  uint64_t k8 = Hacl_Impl_Gen_add_carry(k7, t12, f >> (uint32_t)32U, t1_b);
  uint64_t m = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, f, f << (uint32_t)32U, temp_zl);
  uint64_t uu____7 = Hacl_Impl_Gen_sub_borrow(m, f, f >> (uint32_t)32U, temp_zh);
  uint64_t t22 = t2_b[0U];
  uint64_t t32 = t3_b[0U];
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, t22, temp_zl[0U], t2_b);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, t32, temp_zh[0U], t3_b);
  uint64_t uu____8 = Hacl_Impl_Gen_add_carry(k10, t41, (uint64_t)0U, temp_zl);
  uint64_t t42 = temp_zl[0U];
  return t42;
}

static uint64_t Hacl_Impl_Gen_mm_round3(uint64_t *a, uint64_t t4, uint64_t *tempBuffer)
{
  uint64_t tempBufferLocal[2U] = { 0U };
  uint64_t *temp_zl = tempBufferLocal;
  uint64_t *temp_zh = tempBufferLocal + (uint32_t)1U;
  uint64_t x = a[3U];
  uint64_t t0 = tempBuffer[0U];
  uint64_t t1 = tempBuffer[1U];
  uint64_t t2 = tempBuffer[2U];
  uint64_t t3 = tempBuffer[3U];
  uint64_t *t0_b = tempBuffer;
  uint64_t *t1_b = tempBuffer + (uint32_t)1U;
  uint64_t *t2_b = tempBuffer + (uint32_t)2U;
  uint64_t *t3_b = tempBuffer + (uint32_t)3U;
  uint64_t zl = tempBuffer[8U];
  uint64_t zh = tempBuffer[9U];
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, t0, temp_zl);
  uint64_t f = temp_zl[0U];
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, zh, (uint64_t)0U, t0_b);
  uint64_t zl1 = tempBuffer[12U];
  uint64_t zh1 = tempBuffer[13U];
  uint64_t t01 = t0_b[0U];
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, t01, temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, zh1, (uint64_t)0U, temp_zh);
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t1, t0_b);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, temp_zh[0U], (uint64_t)0U, t1_b);
  uint64_t zl2 = tempBuffer[14U];
  uint64_t zh2 = tempBuffer[15U];
  uint64_t t11 = t1_b[0U];
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, t11, temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, zh2, (uint64_t)0U, temp_zh);
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t2, t1_b);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, temp_zh[0U], (uint64_t)0U, t2_b);
  uint128_t res = (uint128_t)x * x;
  uint64_t zl3 = (uint64_t)res;
  uint64_t zh3 = (uint64_t)(res >> (uint32_t)64U);
  uint64_t t21 = t2_b[0U];
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl3, t21, temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, zh3, (uint64_t)0U, temp_zh);
  uint64_t k6 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t3, t2_b);
  uint64_t uu____6 = Hacl_Impl_Gen_add_carry(k6, temp_zh[0U], (uint64_t)0U, t3_b);
  uint64_t t31 = t3_b[0U];
  uint64_t t02 = t0_b[0U];
  uint64_t t12 = t1_b[0U];
  uint64_t t41 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t31, t4, t3_b);
  uint64_t k7 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t02, f << (uint32_t)32U, t0_b);
  uint64_t k8 = Hacl_Impl_Gen_add_carry(k7, t12, f >> (uint32_t)32U, t1_b);
  uint64_t m = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, f, f << (uint32_t)32U, temp_zl);
  uint64_t uu____7 = Hacl_Impl_Gen_sub_borrow(m, f, f >> (uint32_t)32U, temp_zh);
  uint64_t t22 = t2_b[0U];
  uint64_t t32 = t3_b[0U];
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, t22, temp_zl[0U], t2_b);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, t32, temp_zh[0U], t3_b);
  uint64_t uu____8 = Hacl_Impl_Gen_add_carry(k10, t41, (uint64_t)0U, temp_zl);
  uint64_t t42 = temp_zl[0U];
  return t42;
}

static void Hacl_Impl_Gen_montgomery_square(uint64_t *a, uint64_t *result)
{
  uint64_t tempBuffer[20U] = { 0U };
  uint64_t *temp_zl = tempBuffer;
  uint64_t *temp_zh = tempBuffer + (uint32_t)1U;
  uint64_t *t_buffer = tempBuffer + (uint32_t)2U;
  uint64_t x = a[0U];
  uint64_t a1 = a[1U];
  uint64_t a2 = a[2U];
  uint64_t a3 = a[3U];
  uint128_t res0 = (uint128_t)x * x;
  uint64_t f = (uint64_t)res0;
  uint64_t t0 = (uint64_t)(res0 >> (uint32_t)64U);
  uint128_t res1 = (uint128_t)a1 * x;
  uint64_t zl = (uint64_t)res1;
  uint64_t zh = (uint64_t)(res1 >> (uint32_t)64U);
  tempBuffer[6U] = zl;
  tempBuffer[7U] = zh;
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, t0, temp_zl);
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, zh, (uint64_t)0U, temp_zh);
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f << (uint32_t)32U, temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t01 = temp_zl[0U];
  uint64_t t1 = temp_zh[0U];
  uint128_t res2 = (uint128_t)a2 * x;
  uint64_t zl1 = (uint64_t)res2;
  uint64_t zh1 = (uint64_t)(res2 >> (uint32_t)64U);
  tempBuffer[8U] = zl1;
  tempBuffer[9U] = zh1;
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, t1, temp_zl);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, zh1, (uint64_t)0U, temp_zh);
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f >> (uint32_t)32U, temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t11 = temp_zl[0U];
  uint64_t t2 = temp_zh[0U];
  uint128_t res = (uint128_t)a3 * x;
  uint64_t zl2 = (uint64_t)res;
  uint64_t zh2 = (uint64_t)(res >> (uint32_t)64U);
  tempBuffer[10U] = zl2;
  tempBuffer[11U] = zh2;
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, t2, temp_zl);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, zh2, (uint64_t)0U, temp_zh);
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f, temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t21 = temp_zl[0U];
  uint64_t t3 = temp_zh[0U];
  uint64_t t4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t3, f, temp_zl);
  uint64_t k6 = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, t21, f << (uint32_t)32U, temp_zh);
  uint64_t t31 = temp_zl[0U];
  uint64_t t22 = temp_zh[0U];
  uint64_t k7 = Hacl_Impl_Gen_sub_borrow(k6, t31, f >> (uint32_t)32U, temp_zl);
  uint64_t t32 = temp_zl[0U];
  uint64_t uu____6 = Hacl_Impl_Gen_sub_borrow(k7, t4, (uint64_t)0U, temp_zh);
  uint64_t t41 = temp_zh[0U];
  t_buffer[0U] = t01;
  t_buffer[1U] = t11;
  t_buffer[2U] = t22;
  t_buffer[3U] = t32;
  uint64_t t42 = Hacl_Impl_Gen_mm_round1(a, t41, t_buffer);
  uint64_t t43 = Hacl_Impl_Gen_mm_round2(a, t42, t_buffer);
  uint64_t t44 = Hacl_Impl_Gen_mm_round3(a, t43, t_buffer);
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t t02 = t_buffer[0U];
  uint64_t t12 = t_buffer[1U];
  uint64_t t23 = t_buffer[2U];
  uint64_t t33 = t_buffer[3U];
  uint64_t k8 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t02, t44, r0);
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, t12, (uint64_t)0U - (t44 << (uint32_t)32U), r1);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, t23, (uint64_t)0U - t44, r2);
  uint64_t
  uu____7 = Hacl_Impl_Gen_add_carry(k10, t33, (t44 << (uint32_t)32U) - (t44 << (uint32_t)1U), r3);
}

static uint64_t
Hacl_Spec_P256_MontgomeryMultiplication_mm_round(
  uint64_t x,
  uint64_t *b,
  uint64_t t4,
  uint64_t *tempBuffer
)
{
  uint64_t tempBufferLocal[2U] = { 0U };
  uint64_t *temp_zl = tempBufferLocal;
  uint64_t *temp_zh = tempBufferLocal + (uint32_t)1U;
  uint64_t b0 = b[0U];
  uint64_t b1 = b[1U];
  uint64_t b2 = b[2U];
  uint64_t b3 = b[3U];
  uint64_t t0 = tempBuffer[0U];
  uint64_t t1 = tempBuffer[1U];
  uint64_t t2 = tempBuffer[2U];
  uint64_t t3 = tempBuffer[3U];
  uint64_t *t0_b = tempBuffer;
  uint64_t *t1_b = tempBuffer + (uint32_t)1U;
  uint64_t *t2_b = tempBuffer + (uint32_t)2U;
  uint64_t *t3_b = tempBuffer + (uint32_t)3U;
  uint128_t res = (uint128_t)x * b0;
  uint64_t zl = (uint64_t)res;
  uint64_t zh = (uint64_t)(res >> (uint32_t)64U);
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, t0, temp_zl);
  uint64_t f = temp_zl[0U];
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, zh, (uint64_t)0U, t0_b);
  uint128_t res0 = (uint128_t)x * b1;
  uint64_t zl1 = (uint64_t)res0;
  uint64_t zh1 = (uint64_t)(res0 >> (uint32_t)64U);
  uint64_t t01 = t0_b[0U];
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, t01, temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, zh1, (uint64_t)0U, temp_zh);
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t1, t0_b);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, temp_zh[0U], (uint64_t)0U, t1_b);
  uint128_t res1 = (uint128_t)x * b2;
  uint64_t zl2 = (uint64_t)res1;
  uint64_t zh2 = (uint64_t)(res1 >> (uint32_t)64U);
  uint64_t t11 = t1_b[0U];
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, t11, temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, zh2, (uint64_t)0U, temp_zh);
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t2, t1_b);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, temp_zh[0U], (uint64_t)0U, t2_b);
  uint128_t res2 = (uint128_t)x * b3;
  uint64_t zl3 = (uint64_t)res2;
  uint64_t zh3 = (uint64_t)(res2 >> (uint32_t)64U);
  uint64_t t21 = t2_b[0U];
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl3, t21, temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, zh3, (uint64_t)0U, temp_zh);
  uint64_t k6 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], t3, t2_b);
  uint64_t uu____6 = Hacl_Impl_Gen_add_carry(k6, temp_zh[0U], (uint64_t)0U, t3_b);
  uint64_t t31 = t3_b[0U];
  uint64_t t02 = t0_b[0U];
  uint64_t t12 = t1_b[0U];
  uint64_t t41 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t31, t4, t3_b);
  uint64_t k7 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t02, f << (uint32_t)32U, t0_b);
  uint64_t k8 = Hacl_Impl_Gen_add_carry(k7, t12, f >> (uint32_t)32U, t1_b);
  uint64_t m = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, f, f << (uint32_t)32U, temp_zl);
  uint64_t uu____7 = Hacl_Impl_Gen_sub_borrow(m, f, f >> (uint32_t)32U, temp_zh);
  uint64_t t22 = t2_b[0U];
  uint64_t t32 = t3_b[0U];
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, t22, temp_zl[0U], t2_b);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, t32, temp_zh[0U], t3_b);
  uint64_t uu____8 = Hacl_Impl_Gen_add_carry(k10, t41, (uint64_t)0U, temp_zl);
  uint64_t t42 = temp_zl[0U];
  return t42;
}

static void
Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(
  uint64_t *a,
  uint64_t *b,
  uint64_t *result
)
{
  uint64_t tempBuffer[6U] = { 0U };
  uint64_t *temp_zl = tempBuffer;
  uint64_t *temp_zh = tempBuffer + (uint32_t)1U;
  uint64_t *t_buffer = tempBuffer + (uint32_t)2U;
  uint64_t x = a[0U];
  uint64_t a1 = a[1U];
  uint64_t a2 = a[2U];
  uint64_t a3 = a[3U];
  uint64_t b0 = b[0U];
  uint64_t b1 = b[1U];
  uint64_t b2 = b[2U];
  uint64_t b3 = b[3U];
  uint128_t res0 = (uint128_t)b0 * x;
  uint64_t f = (uint64_t)res0;
  uint64_t t0 = (uint64_t)(res0 >> (uint32_t)64U);
  uint128_t res1 = (uint128_t)b1 * x;
  uint64_t zl = (uint64_t)res1;
  uint64_t zh = (uint64_t)(res1 >> (uint32_t)64U);
  uint64_t k = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl, t0, temp_zl);
  uint64_t uu____0 = Hacl_Impl_Gen_add_carry(k, zh, (uint64_t)0U, temp_zh);
  uint64_t k1 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f << (uint32_t)32U, temp_zl);
  uint64_t uu____1 = Hacl_Impl_Gen_add_carry(k1, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t01 = temp_zl[0U];
  uint64_t t1 = temp_zh[0U];
  uint128_t res2 = (uint128_t)b2 * x;
  uint64_t zl1 = (uint64_t)res2;
  uint64_t zh1 = (uint64_t)(res2 >> (uint32_t)64U);
  uint64_t k2 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl1, t1, temp_zl);
  uint64_t uu____2 = Hacl_Impl_Gen_add_carry(k2, zh1, (uint64_t)0U, temp_zh);
  uint64_t k3 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f >> (uint32_t)32U, temp_zl);
  uint64_t uu____3 = Hacl_Impl_Gen_add_carry(k3, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t11 = temp_zl[0U];
  uint64_t t2 = temp_zh[0U];
  uint128_t res = (uint128_t)b3 * x;
  uint64_t zl2 = (uint64_t)res;
  uint64_t zh2 = (uint64_t)(res >> (uint32_t)64U);
  uint64_t k4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, zl2, t2, temp_zl);
  uint64_t uu____4 = Hacl_Impl_Gen_add_carry(k4, zh2, (uint64_t)0U, temp_zh);
  uint64_t k5 = Hacl_Impl_Gen_add_carry((uint64_t)0U, temp_zl[0U], f, temp_zl);
  uint64_t uu____5 = Hacl_Impl_Gen_add_carry(k5, temp_zh[0U], (uint64_t)0U, temp_zh);
  uint64_t t21 = temp_zl[0U];
  uint64_t t3 = temp_zh[0U];
  uint64_t t4 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t3, f, temp_zl);
  uint64_t k6 = Hacl_Impl_Gen_sub_borrow((uint64_t)0U, t21, f << (uint32_t)32U, temp_zh);
  uint64_t t31 = temp_zl[0U];
  uint64_t t22 = temp_zh[0U];
  uint64_t k7 = Hacl_Impl_Gen_sub_borrow(k6, t31, f >> (uint32_t)32U, temp_zl);
  uint64_t t32 = temp_zl[0U];
  uint64_t uu____6 = Hacl_Impl_Gen_sub_borrow(k7, t4, (uint64_t)0U, temp_zh);
  uint64_t t41 = temp_zh[0U];
  t_buffer[0U] = t01;
  t_buffer[1U] = t11;
  t_buffer[2U] = t22;
  t_buffer[3U] = t32;
  uint64_t t42 = Hacl_Spec_P256_MontgomeryMultiplication_mm_round(a1, b, t41, t_buffer);
  uint64_t t43 = Hacl_Spec_P256_MontgomeryMultiplication_mm_round(a2, b, t42, t_buffer);
  uint64_t t44 = Hacl_Spec_P256_MontgomeryMultiplication_mm_round(a3, b, t43, t_buffer);
  uint64_t *r0 = result;
  uint64_t *r1 = result + (uint32_t)1U;
  uint64_t *r2 = result + (uint32_t)2U;
  uint64_t *r3 = result + (uint32_t)3U;
  uint64_t t02 = t_buffer[0U];
  uint64_t t12 = t_buffer[1U];
  uint64_t t23 = t_buffer[2U];
  uint64_t t33 = t_buffer[3U];
  uint64_t k8 = Hacl_Impl_Gen_add_carry((uint64_t)0U, t02, t44, r0);
  uint64_t k9 = Hacl_Impl_Gen_add_carry(k8, t12, (uint64_t)0U - (t44 << (uint32_t)32U), r1);
  uint64_t k10 = Hacl_Impl_Gen_add_carry(k9, t23, (uint64_t)0U - t44, r2);
  uint64_t
  uu____7 = Hacl_Impl_Gen_add_carry(k10, t33, (t44 << (uint32_t)32U) - (t44 << (uint32_t)1U), r3);
}

static void Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowN(uint32_t n1, uint64_t *a)
{
  for (uint32_t i = (uint32_t)0U; i < n1; i = i + (uint32_t)1U)
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(a, a, a);
}

static void
Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowNminusOne(
  uint32_t n1,
  uint64_t *a,
  uint64_t *b
)
{
  b[0U] = (uint64_t)1U;
  b[1U] = (uint64_t)18446744069414584320U;
  b[2U] = (uint64_t)18446744073709551615U;
  b[3U] = (uint64_t)4294967294U;
  for (uint32_t i = (uint32_t)0U; i < n1; i = i + (uint32_t)1U)
  {
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(b, a, b);
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(a, a, a);
  }
}

static void
Hacl_Spec_P256_MontgomeryMultiplication_exponent(
  uint64_t *a,
  uint64_t *result,
  uint64_t *tempBuffer
)
{
  uint64_t *buffer_norm_1 = tempBuffer;
  uint64_t *buffer_result1 = tempBuffer + (uint32_t)4U;
  uint64_t *buffer_result2 = tempBuffer + (uint32_t)8U;
  uint64_t *buffer_norm_3 = tempBuffer + (uint32_t)12U;
  uint64_t *buffer_result3 = tempBuffer + (uint32_t)16U;
  memcpy(buffer_norm_1, a, (uint32_t)4U * sizeof a[0U]);
  uint64_t *buffer_a = buffer_norm_1;
  uint64_t *buffer_b0 = buffer_norm_1 + (uint32_t)4U;
  Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowNminusOne((uint32_t)32U,
    buffer_a,
    buffer_b0);
  Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)224U, buffer_b0);
  memcpy(buffer_result2, a, (uint32_t)4U * sizeof a[0U]);
  Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)192U, buffer_result2);
  memcpy(buffer_norm_3, a, (uint32_t)4U * sizeof a[0U]);
  uint64_t *buffer_a0 = buffer_norm_3;
  uint64_t *buffer_b = buffer_norm_3 + (uint32_t)4U;
  Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowNminusOne((uint32_t)94U,
    buffer_a0,
    buffer_b);
  Hacl_Spec_P256_MontgomeryMultiplication_fsquarePowN((uint32_t)2U, buffer_b);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    buffer_result2,
    buffer_result1);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    buffer_result3,
    buffer_result1);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(buffer_result1,
    a,
    buffer_result1);
  memcpy(result, buffer_result1, (uint32_t)4U * sizeof buffer_result1[0U]);
}

static void
Hacl_Spec_P256_MontgomeryMultiplication_cswap(uint64_t bit, uint64_t *p1, uint64_t *p2)
{
  uint64_t mask = (uint64_t)0U - bit;
  for (uint32_t i = (uint32_t)0U; i < (uint32_t)12U; i = i + (uint32_t)1U)
  {
    uint64_t dummy = mask & (p1[i] ^ p2[i]);
    p1[i] = p1[i] ^ dummy;
    p2[i] = p2[i] ^ dummy;
  }
}

static void
upl_zer_buffer(
  uint32_t c0,
  uint32_t c1,
  uint32_t c2,
  uint32_t c3,
  uint32_t c4,
  uint32_t c5,
  uint32_t c6,
  uint32_t c7,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c1, c0);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c3, c2);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c5, c4);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c7, c6);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_fir_buffer(
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = (uint64_t)0U;
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c11, (uint32_t)0U);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c13, c12);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c15, c14);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void upl_sec_buffer(uint32_t c12, uint32_t c13, uint32_t c14, uint32_t c15, uint64_t *o)
{
  uint64_t b0 = (uint64_t)0U;
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c12, (uint32_t)0U);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c14, c13);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u((uint32_t)0U, c15);
  o[0U] = b0;
  o[1U] = b1;
  o[2U] = b2;
  o[3U] = b3;
}

static void
upl_thi_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c9, c8);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u((uint32_t)0U, c10);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c15, c14);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_for_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c11,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c10, c9);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c13, c11);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c15, c14);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c8, c13);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_fif_buffer(
  uint32_t c8,
  uint32_t c10,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c12, c11);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u((uint32_t)0U, c13);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c10, c8);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_six_buffer(
  uint32_t c9,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c13, c12);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c15, c14);
  uint64_t b2 = (uint64_t)0U;
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c11, c9);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_sev_buffer(
  uint32_t c8,
  uint32_t c9,
  uint32_t c10,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c14, c13);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c8, c15);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c10, c9);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c12, (uint32_t)0U);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void
upl_eig_buffer(
  uint32_t c9,
  uint32_t c10,
  uint32_t c11,
  uint32_t c12,
  uint32_t c13,
  uint32_t c14,
  uint32_t c15,
  uint64_t *temp,
  uint64_t *o
)
{
  uint64_t b0 = Hacl_Spec_P256_Core_store_high_low_u(c15, c14);
  uint64_t b1 = Hacl_Spec_P256_Core_store_high_low_u(c9, (uint32_t)0U);
  uint64_t b2 = Hacl_Spec_P256_Core_store_high_low_u(c11, c10);
  uint64_t b3 = Hacl_Spec_P256_Core_store_high_low_u(c13, (uint32_t)0U);
  temp[0U] = b0;
  temp[1U] = b1;
  temp[2U] = b2;
  temp[3U] = b3;
  Hacl_Impl_Gen_reduction_prime_2prime_impl(temp, o);
}

static void solinas_reduction_impl(uint64_t *i, uint64_t *o)
{
  uint64_t tempBuffer[36U] = { 0U };
  uint64_t redBuffer[4U] = { 0U };
  uint64_t *t0 = tempBuffer;
  uint64_t *t1 = tempBuffer + (uint32_t)4U;
  uint64_t *t2 = tempBuffer + (uint32_t)8U;
  uint64_t *t3 = tempBuffer + (uint32_t)12U;
  uint64_t *t4 = tempBuffer + (uint32_t)16U;
  uint64_t *t5 = tempBuffer + (uint32_t)20U;
  uint64_t *t6 = tempBuffer + (uint32_t)24U;
  uint64_t *t7 = tempBuffer + (uint32_t)28U;
  uint64_t *t8 = tempBuffer + (uint32_t)32U;
  uint64_t i0 = i[0U];
  uint64_t i1 = i[1U];
  uint64_t i2 = i[2U];
  uint64_t i3 = i[3U];
  uint64_t i4 = i[4U];
  uint64_t i5 = i[5U];
  uint64_t i6 = i[6U];
  uint64_t i7 = i[7U];
  uint32_t c0 = (uint32_t)i0;
  uint32_t c1 = (uint32_t)(i0 >> (uint32_t)32U);
  uint32_t c2 = (uint32_t)i1;
  uint32_t c3 = (uint32_t)(i1 >> (uint32_t)32U);
  uint32_t c4 = (uint32_t)i2;
  uint32_t c5 = (uint32_t)(i2 >> (uint32_t)32U);
  uint32_t c6 = (uint32_t)i3;
  uint32_t c7 = (uint32_t)(i3 >> (uint32_t)32U);
  uint32_t c8 = (uint32_t)i4;
  uint32_t c9 = (uint32_t)(i4 >> (uint32_t)32U);
  uint32_t c10 = (uint32_t)i5;
  uint32_t c11 = (uint32_t)(i5 >> (uint32_t)32U);
  uint32_t c12 = (uint32_t)i6;
  uint32_t c13 = (uint32_t)(i6 >> (uint32_t)32U);
  uint32_t c14 = (uint32_t)i7;
  uint32_t c15 = (uint32_t)(i7 >> (uint32_t)32U);
  upl_zer_buffer(c0, c1, c2, c3, c4, c5, c6, c7, redBuffer, t0);
  upl_fir_buffer(c11, c12, c13, c14, c15, redBuffer, t1);
  upl_sec_buffer(c12, c13, c14, c15, t2);
  upl_thi_buffer(c8, c9, c10, c14, c15, redBuffer, t3);
  upl_for_buffer(c8, c9, c10, c11, c13, c14, c15, redBuffer, t4);
  upl_fif_buffer(c8, c10, c11, c12, c13, redBuffer, t5);
  upl_six_buffer(c9, c11, c12, c13, c14, c15, redBuffer, t6);
  upl_sev_buffer(c8, c9, c10, c12, c13, c14, c15, redBuffer, t7);
  upl_eig_buffer(c9, c10, c11, c12, c13, c14, c15, redBuffer, t8);
  Hacl_Impl_Gen_p256_double(t2, t2);
  Hacl_Impl_Gen_p256_double(t1, t1);
  Hacl_Impl_Gen_p256_add(t0, t1, o);
  Hacl_Impl_Gen_p256_add(t2, o, o);
  Hacl_Impl_Gen_p256_add(t3, o, o);
  Hacl_Impl_Gen_p256_add(t4, o, o);
  Hacl_Impl_Gen_p256_sub(o, t5, o);
  Hacl_Impl_Gen_p256_sub(o, t6, o);
  Hacl_Impl_Gen_p256_sub(o, t7, o);
  Hacl_Impl_Gen_p256_sub(o, t8, o);
}

void pointToDomain(uint64_t *p, uint64_t *result)
{
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *r_x = result;
  uint64_t *r_y = result + (uint32_t)4U;
  uint64_t *r_z = result + (uint32_t)8U;
  uint64_t multBuffer[8U] = { 0U };
  Hacl_Impl_Gen_shift_256_impl(p_x, multBuffer);
  solinas_reduction_impl(multBuffer, r_x);
  uint64_t multBuffer0[8U] = { 0U };
  Hacl_Impl_Gen_shift_256_impl(p_y, multBuffer0);
  solinas_reduction_impl(multBuffer0, r_y);
  uint64_t multBuffer1[8U] = { 0U };
  Hacl_Impl_Gen_shift_256_impl(p_z, multBuffer1);
  solinas_reduction_impl(multBuffer1, r_z);
}

static void fromDomain(uint64_t *f, uint64_t *result)
{
  uint64_t one1[4U] = { 0U };
  one1[0U] = (uint64_t)1U;
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(f, one1, result);
}

void pointFromDomain(uint64_t *p, uint64_t *result)
{
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *r_x = result;
  uint64_t *r_y = result + (uint32_t)4U;
  uint64_t *r_z = result + (uint32_t)8U;
  fromDomain(p_x, r_x);
  fromDomain(p_y, r_y);
  fromDomain(p_z, r_z);
}

static void quatre(uint64_t *a, uint64_t *result)
{
  Hacl_Impl_Gen_montgomery_square(a, result);
  Hacl_Impl_Gen_montgomery_square(result, result);
}

static void multByTwo(uint64_t *a, uint64_t *out)
{
  uint64_t a0 = a[0U];
  uint64_t a1 = a[1U];
  uint64_t a2 = a[2U];
  uint64_t a3 = a[3U];
  uint64_t *r0 = out;
  uint64_t *r1 = out + (uint32_t)1U;
  uint64_t *r2 = out + (uint32_t)2U;
  uint64_t *r3 = out + (uint32_t)3U;
  uint64_t cc = Hacl_Impl_Gen_add_carry((uint64_t)0U, a0, a0, r0);
  uint64_t cc1 = Hacl_Impl_Gen_add_carry(cc, a1, a1, r1);
  uint64_t cc2 = Hacl_Impl_Gen_add_carry(cc1, a2, a2, r2);
  uint64_t cc3 = Hacl_Impl_Gen_add_carry(cc2, a3, a3, r3);
  uint64_t t = cc3;
  uint64_t cc4 = Hacl_Impl_Gen_add_carry(cc3, r0[0U], (uint64_t)0U, r0);
  uint64_t cc5 = Hacl_Impl_Gen_add_carry(cc4, r1[0U], (uint64_t)0U - (t << (uint32_t)32U), r1);
  uint64_t cc6 = Hacl_Impl_Gen_add_carry(cc5, r2[0U], (uint64_t)0U - t, r2);
  uint64_t
  uu____0 = Hacl_Impl_Gen_add_carry(cc6, r3[0U], (t << (uint32_t)32U) - (t << (uint32_t)1U), r3);
}

static uint64_t isZero_uint64(uint64_t *f)
{
  uint64_t a0 = f[0U];
  uint64_t a1 = f[1U];
  uint64_t a2 = f[2U];
  uint64_t a3 = f[3U];
  uint64_t r0 = FStar_UInt64_eq_mask(a0, (uint64_t)0U);
  uint64_t r1 = FStar_UInt64_eq_mask(a1, (uint64_t)0U);
  uint64_t r2 = FStar_UInt64_eq_mask(a2, (uint64_t)0U);
  uint64_t r3 = FStar_UInt64_eq_mask(a3, (uint64_t)0U);
  uint64_t r01 = r0 & r1;
  uint64_t r23 = r2 & r3;
  return r01 & r23;
}

void point_double(uint64_t *p, uint64_t *result, uint64_t *tempBuffer)
{
  uint64_t *s = tempBuffer;
  uint64_t *m = tempBuffer + (uint32_t)4U;
  uint64_t *buffer_for_s_m = tempBuffer + (uint32_t)8U;
  uint64_t *buffer_for_x3 = tempBuffer + (uint32_t)32U;
  uint64_t *buffer_for_y3 = tempBuffer + (uint32_t)40U;
  uint64_t *pypz = tempBuffer + (uint32_t)56U;
  uint64_t *x3 = tempBuffer + (uint32_t)60U;
  uint64_t *y3 = tempBuffer + (uint32_t)64U;
  uint64_t *z3 = tempBuffer + (uint32_t)68U;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t *px = p;
  uint64_t *py = p + (uint32_t)4U;
  uint64_t *pz = p + (uint32_t)8U;
  uint64_t *yy = buffer_for_s_m;
  uint64_t *xyy = buffer_for_s_m + (uint32_t)4U;
  uint64_t *zzzz = buffer_for_s_m + (uint32_t)8U;
  uint64_t *minThreeZzzz = buffer_for_s_m + (uint32_t)12U;
  uint64_t *xx = buffer_for_s_m + (uint32_t)16U;
  uint64_t *threeXx = buffer_for_s_m + (uint32_t)20U;
  Hacl_Impl_Gen_montgomery_square(py, yy);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(px, yy, xyy);
  multByTwo(xyy, s);
  multByTwo(s, s);
  quatre(pz, zzzz);
  multByTwo(zzzz, minThreeZzzz);
  Hacl_Impl_Gen_p256_add(zzzz, minThreeZzzz, minThreeZzzz);
  uint64_t zeros[4U] = { 0U };
  Hacl_Impl_Gen_p256_sub(zeros, minThreeZzzz, minThreeZzzz);
  Hacl_Impl_Gen_montgomery_square(px, xx);
  multByTwo(xx, threeXx);
  Hacl_Impl_Gen_p256_add(xx, threeXx, threeXx);
  Hacl_Impl_Gen_p256_add(minThreeZzzz, threeXx, m);
  uint64_t *twoS = buffer_for_x3;
  uint64_t *mm = buffer_for_x3 + (uint32_t)4U;
  multByTwo(s, twoS);
  Hacl_Impl_Gen_montgomery_square(m, mm);
  Hacl_Impl_Gen_p256_sub(mm, twoS, x3);
  uint64_t *yyyy = buffer_for_y3;
  uint64_t *eightYyyy = buffer_for_y3 + (uint32_t)4U;
  uint64_t *sx3 = buffer_for_y3 + (uint32_t)8U;
  uint64_t *msx3 = buffer_for_y3 + (uint32_t)12U;
  quatre(p_y, yyyy);
  multByTwo(yyyy, eightYyyy);
  multByTwo(eightYyyy, eightYyyy);
  multByTwo(eightYyyy, eightYyyy);
  Hacl_Impl_Gen_p256_sub(s, x3, sx3);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(m, sx3, msx3);
  Hacl_Impl_Gen_p256_sub(msx3, eightYyyy, y3);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(p_y, p_z, pypz);
  multByTwo(pypz, z3);
  memcpy(result, x3, (uint32_t)4U * sizeof x3[0U]);
  memcpy(result + (uint32_t)4U, y3, (uint32_t)4U * sizeof y3[0U]);
  memcpy(result + (uint32_t)8U, z3, (uint32_t)4U * sizeof z3[0U]);
}

static void
copy_point_conditional(
  uint64_t *x3_out,
  uint64_t *y3_out,
  uint64_t *z3_out,
  uint64_t *p,
  uint64_t *maskPoint
)
{
  uint64_t *z = maskPoint + (uint32_t)8U;
  uint64_t mask = isZero_uint64(z);
  uint64_t *p_x = p;
  uint64_t *p_y = p + (uint32_t)4U;
  uint64_t *p_z = p + (uint32_t)8U;
  uint64_t out_0 = x3_out[0U];
  uint64_t out_10 = x3_out[1U];
  uint64_t out_20 = x3_out[2U];
  uint64_t out_30 = x3_out[3U];
  uint64_t x_00 = p_x[0U];
  uint64_t x_10 = p_x[1U];
  uint64_t x_20 = p_x[2U];
  uint64_t x_30 = p_x[3U];
  uint64_t out_010 = out_0;
  uint64_t out_110 = out_10;
  uint64_t out_210 = out_20;
  uint64_t out_310 = out_30;
  uint64_t x_010 = x_00;
  uint64_t x_110 = x_10;
  uint64_t x_210 = x_20;
  uint64_t x_310 = x_30;
  uint64_t r_00 = out_010 ^ mask & (out_010 ^ x_010);
  uint64_t r_10 = out_110 ^ mask & (out_110 ^ x_110);
  uint64_t r_20 = out_210 ^ mask & (out_210 ^ x_210);
  uint64_t r_30 = out_310 ^ mask & (out_310 ^ x_310);
  uint64_t temp_00 = r_00;
  uint64_t temp_10 = r_10;
  uint64_t temp_20 = r_20;
  uint64_t temp_30 = r_30;
  x3_out[0U] = temp_00;
  x3_out[1U] = temp_10;
  x3_out[2U] = temp_20;
  x3_out[3U] = temp_30;
  uint64_t out_00 = y3_out[0U];
  uint64_t out_12 = y3_out[1U];
  uint64_t out_22 = y3_out[2U];
  uint64_t out_32 = y3_out[3U];
  uint64_t x_02 = p_y[0U];
  uint64_t x_12 = p_y[1U];
  uint64_t x_22 = p_y[2U];
  uint64_t x_32 = p_y[3U];
  uint64_t out_011 = out_00;
  uint64_t out_111 = out_12;
  uint64_t out_211 = out_22;
  uint64_t out_311 = out_32;
  uint64_t x_011 = x_02;
  uint64_t x_111 = x_12;
  uint64_t x_211 = x_22;
  uint64_t x_311 = x_32;
  uint64_t r_01 = out_011 ^ mask & (out_011 ^ x_011);
  uint64_t r_11 = out_111 ^ mask & (out_111 ^ x_111);
  uint64_t r_21 = out_211 ^ mask & (out_211 ^ x_211);
  uint64_t r_31 = out_311 ^ mask & (out_311 ^ x_311);
  uint64_t temp_01 = r_01;
  uint64_t temp_11 = r_11;
  uint64_t temp_21 = r_21;
  uint64_t temp_31 = r_31;
  y3_out[0U] = temp_01;
  y3_out[1U] = temp_11;
  y3_out[2U] = temp_21;
  y3_out[3U] = temp_31;
  uint64_t out_02 = z3_out[0U];
  uint64_t out_1 = z3_out[1U];
  uint64_t out_2 = z3_out[2U];
  uint64_t out_3 = z3_out[3U];
  uint64_t x_0 = p_z[0U];
  uint64_t x_1 = p_z[1U];
  uint64_t x_2 = p_z[2U];
  uint64_t x_3 = p_z[3U];
  uint64_t out_01 = out_02;
  uint64_t out_11 = out_1;
  uint64_t out_21 = out_2;
  uint64_t out_31 = out_3;
  uint64_t x_01 = x_0;
  uint64_t x_11 = x_1;
  uint64_t x_21 = x_2;
  uint64_t x_31 = x_3;
  uint64_t r_0 = out_01 ^ mask & (out_01 ^ x_01);
  uint64_t r_1 = out_11 ^ mask & (out_11 ^ x_11);
  uint64_t r_2 = out_21 ^ mask & (out_21 ^ x_21);
  uint64_t r_3 = out_31 ^ mask & (out_31 ^ x_31);
  uint64_t temp_0 = r_0;
  uint64_t temp_1 = r_1;
  uint64_t temp_2 = r_2;
  uint64_t temp_3 = r_3;
  z3_out[0U] = temp_0;
  z3_out[1U] = temp_1;
  z3_out[2U] = temp_2;
  z3_out[3U] = temp_3;
}

static uint64_t compare_felem(uint64_t *a, uint64_t *b)
{
  uint64_t r0 = FStar_UInt64_eq_mask(a[0U], b[0U]);
  uint64_t r1 = FStar_UInt64_eq_mask(a[1U], b[1U]);
  uint64_t r2 = FStar_UInt64_eq_mask(a[2U], b[2U]);
  uint64_t r3 = FStar_UInt64_eq_mask(a[3U], b[3U]);
  return r0 & r1 & r2 & r3;
}

void point_add(uint64_t *p, uint64_t *q, uint64_t *result, uint64_t *tempBuffer)
{
  uint64_t *z1 = p + (uint32_t)8U;
  uint64_t *z2 = q + (uint32_t)8U;
  uint64_t *tempBuffer16 = tempBuffer;
  uint64_t *u11 = tempBuffer + (uint32_t)16U;
  uint64_t *u2 = tempBuffer + (uint32_t)20U;
  uint64_t *s1 = tempBuffer + (uint32_t)24U;
  uint64_t *s2 = tempBuffer + (uint32_t)28U;
  uint64_t *h = tempBuffer + (uint32_t)32U;
  uint64_t *r = tempBuffer + (uint32_t)36U;
  uint64_t *uh = tempBuffer + (uint32_t)40U;
  uint64_t *hCube = tempBuffer + (uint32_t)44U;
  uint64_t *tempBuffer28 = tempBuffer + (uint32_t)60U;
  uint64_t *x1 = p;
  uint64_t *y1 = p + (uint32_t)4U;
  uint64_t *z11 = p + (uint32_t)8U;
  uint64_t *x2 = q;
  uint64_t *y2 = q + (uint32_t)4U;
  uint64_t *z210 = q + (uint32_t)8U;
  uint64_t *z2Square = tempBuffer16;
  uint64_t *z1Square = tempBuffer16 + (uint32_t)4U;
  uint64_t *z2Cube = tempBuffer16 + (uint32_t)8U;
  uint64_t *z1Cube = tempBuffer16 + (uint32_t)12U;
  Hacl_Impl_Gen_montgomery_square(z210, z2Square);
  Hacl_Impl_Gen_montgomery_square(z11, z1Square);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z2Square,
    z210,
    z2Cube);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z1Square,
    z11,
    z1Cube);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(x1, z2Square, u11);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(x2, z1Square, u2);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(y1, z2Cube, s1);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(y2, z1Cube, s2);
  uint64_t one1 = compare_felem(u11, u2);
  uint64_t two = compare_felem(s1, s2);
  uint64_t z1notZero = isZero_uint64(z1);
  uint64_t z2notZero = isZero_uint64(z2);
  uint64_t pointsInf = ~z1notZero & ~z2notZero;
  uint64_t onetwo = one1 & two;
  uint64_t result1 = onetwo & pointsInf;
  bool flag = result1 == (uint64_t)0xffffffffffffffffU;
  if (flag)
    point_double(p, result, tempBuffer);
  else
  {
    uint64_t *temp = tempBuffer16;
    Hacl_Impl_Gen_p256_sub(u2, u11, h);
    Hacl_Impl_Gen_p256_sub(s2, s1, r);
    Hacl_Impl_Gen_montgomery_square(h, temp);
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(u11, temp, uh);
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(h, temp, hCube);
    uint64_t *z11 = p + (uint32_t)8U;
    uint64_t *z21 = q + (uint32_t)8U;
    uint64_t *tempBuffer161 = tempBuffer28;
    uint64_t *x3_out1 = tempBuffer28 + (uint32_t)16U;
    uint64_t *y3_out1 = tempBuffer28 + (uint32_t)20U;
    uint64_t *z3_out1 = tempBuffer28 + (uint32_t)24U;
    uint64_t *rSquare = tempBuffer161;
    uint64_t *r_h = tempBuffer161 + (uint32_t)4U;
    uint64_t *twouh = tempBuffer161 + (uint32_t)8U;
    Hacl_Impl_Gen_montgomery_square(r, rSquare);
    Hacl_Impl_Gen_p256_sub(rSquare, hCube, r_h);
    multByTwo(uh, twouh);
    Hacl_Impl_Gen_p256_sub(r_h, twouh, x3_out1);
    uint64_t *s1hCube = tempBuffer161;
    uint64_t *u1hx3 = tempBuffer161 + (uint32_t)4U;
    uint64_t *ru1hx3 = tempBuffer161 + (uint32_t)8U;
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(s1, hCube, s1hCube);
    Hacl_Impl_Gen_p256_sub(uh, x3_out1, u1hx3);
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(r, u1hx3, ru1hx3);
    Hacl_Impl_Gen_p256_sub(ru1hx3, s1hCube, y3_out1);
    uint64_t *z1z2 = tempBuffer161;
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z11, z21, z1z2);
    Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(h, z1z2, z3_out1);
    copy_point_conditional(x3_out1, y3_out1, z3_out1, q, p);
    copy_point_conditional(x3_out1, y3_out1, z3_out1, p, q);
    memcpy(result, x3_out1, (uint32_t)4U * sizeof x3_out1[0U]);
    memcpy(result + (uint32_t)4U, y3_out1, (uint32_t)4U * sizeof y3_out1[0U]);
    memcpy(result + (uint32_t)8U, z3_out1, (uint32_t)4U * sizeof z3_out1[0U]);
  }
}

void norm(uint64_t *p, uint64_t *resultPoint, uint64_t *tempBuffer)
{
  uint64_t *xf = p;
  uint64_t *yf = p + (uint32_t)4U;
  uint64_t *zf = p + (uint32_t)8U;
  uint64_t *resultX = resultPoint;
  uint64_t *resultY = resultPoint + (uint32_t)4U;
  uint64_t *resultZ = resultPoint + (uint32_t)8U;
  uint64_t *z2f = tempBuffer + (uint32_t)4U;
  uint64_t *z3f = tempBuffer + (uint32_t)8U;
  uint64_t *tempBuffer20 = tempBuffer + (uint32_t)12U;
  Hacl_Impl_Gen_montgomery_square(zf, z2f);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(z2f, zf, z3f);
  Hacl_Spec_P256_MontgomeryMultiplication_exponent(z2f, z2f, tempBuffer20);
  Hacl_Spec_P256_MontgomeryMultiplication_exponent(z3f, z3f, tempBuffer20);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(xf, z2f, z2f);
  Hacl_Spec_P256_MontgomeryMultiplication_montgomery_multiplication_buffer(yf, z3f, z3f);
  fromDomain(z2f, resultX);
  fromDomain(z3f, resultY);
  resultZ[0U] = (uint64_t)1U;
  resultZ[1U] = (uint64_t)0U;
  resultZ[2U] = (uint64_t)0U;
  resultZ[3U] = (uint64_t)0U;
}

void
montgomery_ladder(
  uint64_t *p,
  uint64_t *q,
  uint32_t scalarSize,
  uint8_t *scalar,
  uint64_t *tempBuffer
)
{
  for (uint32_t i = (uint32_t)0U; i < scalarSize; i = i + (uint32_t)1U)
  {
    uint32_t bit = scalarSize - i - (uint32_t)(krml_checked_int_t)1;
    uint64_t bit1 = (uint64_t)(scalar[bit / (uint32_t)8U] >> bit % (uint32_t)8U & (uint8_t)1U);
    Hacl_Spec_P256_MontgomeryMultiplication_cswap(bit1, p, q);
    point_add(q, p, q, tempBuffer);
    point_double(p, p, tempBuffer);
    Hacl_Spec_P256_MontgomeryMultiplication_cswap(bit1, p, q);
  }
}

void
scalarMultiplication(
  uint64_t *p,
  uint64_t *result,
  uint32_t scalarSize,
  uint8_t *scalar,
  uint64_t *tempBuffer
)
{
  uint32_t scalarSize1 = scalarSize * (uint32_t)(krml_checked_int_t)8;
  pointToDomain(p, result);
  uint64_t *q = tempBuffer;
  uint64_t *buff = tempBuffer + (uint32_t)12U;
  montgomery_ladder(q, result, scalarSize1, scalar, buff);
  norm(q, result, buff);
}

